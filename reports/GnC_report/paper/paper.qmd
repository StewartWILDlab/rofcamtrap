---
title: "Using Camera Traps and Computer Vision to Quantify Wildlife Diversity and Co-occurrence Where the Hudsons's Bay Lowlands meets the Ontario Shield"
subtitle: "Prepared for: Canadian Wildlife Service, Canadian Forest Service, and Wilfrid Laurier University - 2024 update"
author:
  - name: Valentin Lucet
    email: valentin.lucet@gmail.com
    orcid: 0000-0003-0268-818X
    affiliation: 
      - ref: wlu
  - name: Samantha McFarlane
    affiliation:
      - ref: eccc
  - name: Meghna Pal
    affiliation:
      - ref: wlu
  - name: Jennifer Baltzer
    affiliation:
      - ref: wlu
  - name: Cheryl A. Johnson
    affiliation:
      - ref: eccc
  - name: Eric Neilson
    affiliation:
      - ref: nrcan
  - name: Philip Weibe
    affiliation:
      - ref: nrcan
  - name: Frances Stewart
    affiliation:
      - ref: wlu
  - name: Josie Hughes
    affiliation:
      - ref: eccc
affiliations:
  - id: wlu
    name: Wilfrid Laurier University
  - id: eccc
    name: Environment & Climate Change Canada
  - id: nrcan
    name: Natural Resources Canada
title-block-published: "Last updated"  
date: now
date-format: long
format:
  # html: default
  pdf:
    toc: true
    number-sections: true
    number-offset: 1
    colorlinks: true
    extra_dependencies: ["float"]
format-links: false
execute:
  echo: true
  warning: false
  message: false
  comment: "#>"
  fig-path: "../figures/"
  fig-dpi: 600
bibliography: references.bib
abstract: >
  Northern Ontario holds some of the world’s last intact wild places, but it is under pressure from resource extraction and climate change. Our understanding of biodiversity measures like species occurrence, abundance, diversity, and density for the region are lacking, yet needed for strategic decision-making about the impacts of resource extraction on ecosystem services and biodiversity. The use of non-invasive survey methods, such as camera trap arrays, has significantly changed our ability to quantify wildlife across large and remote regions. However, these methods generate a large amount of data and require tools to speed up the identification of species. Machine learning methods, such as deep neural networks, are a promising tool to overcome this data bottle-neck. Yet, variables like strong seasonality, changes in vegetation type, and species characteristics can strongly impact their efficacy. The extent to which these methods perform for northern regions is not well established. Using an array of cameras deployed across an heterogeneous sampling area straddling two Northern Ontario ecozones (Hudson Bay Lowlands and Ontario Shield), we first look to quantify current species diversity and co-occurrence.In 2023 we developed an open source workflow for camera trap image identification, access, and storage. We have updated it in 2024 for efficiency. We also continue to investigate the requirements to develop a machine learning image classifier. {{< pagebreak >}}

# keywords: |
#   keyword 1; keyword 2; keyword 3
# highlights: |
#   These are the highlights. 
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
backup_options <- options()
options(scipen = 1, digits = 2)
do_eval <- FALSE
source(here::here("scripts", "R", "postprocess", "3_ordination.R"))
```

{{< pagebreak >}}

# Introduction

Northern Ontario holds some of the world’s last intact wild places, but it is under pressure from resource extraction and global warming. Our understanding of basic biodiversity measures like species occurrence, abundance, diversity, and density for the region are still rudimental but needed to weigh ecological-economical decision tradeoffs. Traditionally, quantifying and monitoring biodiversity in northern regions required expensive and time-consuming field surveys. Today, the use of non-intrusive methods, such as arrays of 100s of camera traps, has significantly changed field practices for reliably quantifying wildlife across large and remote regions. However, these methods generate a large amount of data and require tools to speed up the identification, and processing, of image datasets.

Machine learning methods, such as deep neural networks, are now in use to automate the process of species identification and help in that endeavor. We aim to evaluate the viability of deep neural networks for camera trap surveys currently taking place in Northern Ontario’s heterogeneous landscapes, including the Hudson Bay Lowlands and Ontario Shield ecozones, as variables like weather, seasonality, vegetation type, and species characteristics can impact the performance of these methods. The extent to which these methods perform for northern regions and northern species is not well established. There are currently no available machine learning models for identifying species from images in northern regions and developing one would be beneficial as a monitoring and modeling tool for northern boreal ecosystems.

Using an array of cameras deployed in a heterogeneous sampling area straddling two of Northern Ontario’s ecozones (Hudson Bay Lowlands and Ontario Shield), we first look to quantify current species diversity and co-occurrence. We’ve recently developed an open-source workflow for camera trap image identification, access, and storage, which we are using to identify images, with the next steps involving the development of an image classifier.

## Update for the 2024 progress report

Since the last progress report (2023 report), we have flagged the following improvements and changes, which are detailed in the new sections of this report.

- The number of fully processed images doubled from about 50000 to more than 100000, keeping with the rate of progress of the first year of tagging.
- We made a first attempt at using machine learning methods to train a species classifier using about 23000 images as the basis dataset, which produced encouraging results.
- After reevaluation of the ability of our current workflow and platform to scale to multiple years of data, we have transitioned the image tagging workflow to [WildTrax](https://WildTrax.ca/).
- This report repeatedly mentions the AI model MegaDetector. Although this model now falls under the toolset known as PytorchWildlife, as of writing in April 2024, it is still commonly known as MegaDetector by the camera trapping community.

### Outputs of note

1. Poster submitted and presented at the 2023 conference of the Ontario chapter of The Wildlife Society (OCTWS).
2. Poster submitted and displayed at the Mushkegowuk Council 2023 Research Meeting.
3. Application to participate in the Workshop on Computer Vision Methods for Ecology at the Caltech Resnick Sustainability Institute.

# Methods

```{r echo=FALSE}
#| label: load-data
anns_wide <- readRDS(here::here("data", "objects", "annotations_wide_treated.rds"))
anns_wide_noempty  <- readRDS(here::here("data", "objects", "annotations_wide_noempty.rds"))
dets <- readRDS(here::here("data", "objects", "detections.rds"))
exif <- readr::read_csv(here::here("data", "tabular", "exif_processed.csv"))
cams_sf_coords <- readRDS(here::here("data", "objects", "cams_sf_coords.rds"))
zones_filtered  <- readRDS(here::here("data", "objects", "zones_filtered.rds"))
zones_pols <- readRDS(here::here("data", "objects", "zones_pols.rds"))
base_map <- readRDS(here::here("data", "objects", "base_map.rds"))
ont_base_map <- readRDS(here::here("data", "objects", "ont_base_map.rds"))
camera_days <- readRDS(here::here("data", "objects" ,"camera_days.rds"))
camera_regions <- readRDS(here::here("data", "objects" ,"camera_regions.rds"))
extr_table_sub_tb <- readRDS(here::here("data", "objects" ,"extr_table_sub_tb.rds"))
anns_wide_animals_joined_mut <- readRDS(here::here("data", "objects", "annotations_animals.rds"))
anns_wide_animals_table_tb <- readRDS(here::here("data", "objects" ,"anns_wide_animals_table_tb.rds"))
events_tb_filt_with_col <- readRDS(here::here("data", "objects" ,"events_tb_filt_with_col.rds"))
events_cam_days <- readRDS(here::here("data", "objects" ,"events_cam_days.rds"))
events_tb_cam <- readRDS(here::here("data", "objects" ,"events_tb_cam.rds"))
sci_names_processed <- readRDS(here::here("data", "objects" ,"sci_names_processed.rds"))
table_cat <- readRDS(here::here("data", "objects" ,"table_cat.rds"))
```

## Camera deployment methods

The camera deployment protocol follows a spatially balanced hierarchical design that provides an even distribution of sample sites across biotic and abiotic conditions within the study area (@vanDamBates2018). The study sites span three ecoregions (Northern Taiga, James Bay, Big Trout Lake) and two ecozones (Hudson Bay Lowlands, Ontario Shield). The network was established by CWS in 2020 and seasonally equipped in a rolling design with 500 autonomous recording units (ARUs) to monitor audible species diversity paired with site-specific on-ground vegetation imagery.

In March through Sepember of 2022, 272 unbaited and unlured wildlife camera traps ([Reconyx Hyper Fire II](https://reconyx.com/product/hyperfire-2-covert-ir-camera)) were added to this study design, spread over 60 plots and 227 sites. The cameras were deployed by CWS Ontario staff and contractors along linear features such as game trails, along streams and wetlands, on shorelines of small lakes, or facing beaver lodges and dams. All cameras were programmed to take bursts of 5 images (one per second) at each trigger of their motion sensor.

Instructions for determining where to set up the camera included looking for signs of animal presence, such as scat, scratch marks, tracks, and ungulate carcasses; as well as targeting landscape features likely to attract wildlife, such as game trails, riparian corridors, ridgelines, or vegetation. The cameras were enclosed in a cage (Security Enclosure) and attached to a tree or post overlooking the area, making sure that the attachment point could support the camera’s weight during strong winds. The cameras were placed about 1.2 m above the ground and angled slightly downwards, with the target detection zone about 5 m away. If the snow depth was more than 1.2 m at the time of deployment, the camera was raised to a height of 1.5 m instead. The cameras were set up to have an unobstructed view, at least 5 m wide and 10 m long, of the detection zone. Ideally, they were placed facing either north, northwest, or northeast to reduce glare from direct sunlight. After the activation of the camera, the lens was checked for cleanliness, and test photos were taken to ensure the camera was working correctly.

## Deployment summary

Briefly, 5 ARUs were deployed per site. Up to 4 cameras were paired with ARUs at each of these sites, with at least one of these cameras placed at a random ARU site and one other prioritized to focus on a game trail or open area to increase detection probability. Weather restrictions and field conditions limited access to some sites resulting in some variation in this design. In total, we have 43 plots with 4 cameras, 5 plots with 3 cameras, and 3 plots with 2 cameras. @fig-deployment shows the location of the clustered cameras and their deployment and retrieval dates. 

  - 59 cameras were deployed in March 2022, 53 of which were retrieved in September 2022. The remaining 6 cameras meant to be retrieved then were instead retrieved in March 2023 due to logistical and accessibility issues.
  
  - 116 cameras were deployed in May 2022, and retrieved in September 2022.
  
  - 18 cameras were deployed in June 2022, and retrieved in September 2023
  
  - 79 cameras were deployed in September 2022, and 77 were retrieved in March 2023. The remaining 3 have not been retrieved yet due to weather and logistical constraints.

The work conducted under this project focuses on developing an open-source workflow to quantify the wildlife species detected in those images that have been retrieved to date. This workflow is also being designed to include anticipated additional images from future deployments on this, and potentially other, projects.

```{r echo=FALSE}
#| label: make-labels

regions_labels <- zones_filtered |> 
  dplyr::mutate(ECOZONE_NA = c("Hudson Bay\nLowlands", "Ontario\nShield")) |> 
  sf::st_drop_geometry() |> 
  dplyr::bind_cols(data.frame(x = c(-84.5, -88.75),
                              y = c(52, 52.6)))
```

```{r echo=FALSE}
#| label: fig-deployment
#| fig-cap: "Left: Map of the province of Ontario with ecoregion and inset map delimitation. Right: Inset map of camera deployment locations as well as deployment and retrieval dates for the 227 sites. Labels for the 2 ecoregions spanned by the sampling area are included."
#| fig.width: 8
#| fig.height: 6

p <- base_map +
  ggplot2::geom_label(ggplot2::aes(x = x, y = y,label = ECOZONE_NA), inherit.aes = FALSE,
                      size = 2.5, data = regions_labels) +
  ggplot2::geom_point(data = cams_sf_coords,
                      shape = 24, col = "black", #fill = "red",
                      ggplot2::aes(x = X, y = Y, fill = deployment_group), 
                      inherit.aes = FALSE) +
  ggplot2::labs(fill = "Deployment - retrieval") #+
  # ggplot2::theme_void() +
  # ggplot2::theme(
  #   # legend.justification defines the edge of the legend that the legend.position coordinates refer to
  #   legend.justification = c(0, 1),
  #   # Set the legend flush with the left side of the plot, and just slightly below the top of the plot
  #   legend.position = c(0, .95)
  #   ) +
  # ggplot2::theme(
  #   text = ggplot2::element_text(family = "Futura-Medium"),
  #   legend.title = ggplot2::element_text(family = "Futura-Bold", size = 10),
  #   legend.text = ggplot2::element_text(family = "Futura-Medium", size = 10)
  #   ) +
  

ggplot2::ggsave(plot = p, filename = here::here("figures", "camera_array.png"), 
                width = 6, height = 8, dpi = 300)

# saveRDS(p, here::here("data", "objects", "camera_array.rds"))
# p <- readRDS(here::here("data", "objects", "camera_array.rds"))

cowplot::plot_grid(ont_base_map, p, ncol = 2, rel_widths = c(1,2))
```

## Data access and storage

```{r echo=FALSE}
#| label: basic-stats-zero

# Compute total amount of images
nb_image <- length(unique(dets$image_id))
nb_image_fmt <- prettyNum(nb_image, big.mark = ",")

# Hardcoded check
stopifnot(nb_image == 3288178) # 2788375
```

Images were retrieved from all cameras using a 32 GB SD card and copied onto a 10 TB hard drive (LaCie d2 Professional). The images amounted to 2+TB of memory on this disk, for a total of **`r nb_image_fmt`** images. They were stored with the following directory structure: 

-   Plot id (e.g. P128)
-   Site id (e.g. P128-1)
-   Camera id (e.g. CFS-10, with the first 3 letters corresponding to the group owning the camera: CFS for Canadian Forest Service, WLU for Wilfrid Laurier University)
-   DCIM folder (created by the camera)
-   Overflow folder (e.g. 100RECNX, 101RECNX ; each such folder contains up to 10,000 images)

We have made a backup of these images on a Compute Canada cloud instance, accessible via SSH terminal. We also plan to keep an additional copy of the images on a separate hard drive.

## Image processing

In this section, we separate the workflow we developed and used between September 2022 and December 2023, from the new workflow we are experimenting with and hoping to perfect for 2024 and onward. In 2022 we explored alternative tagging methods as no platform was at that time equipped for our needs. We developed a custom and reproducible platform for image tagging through LabelStudio and R scripts. We deployed our own LabelStudio platform and used custom (yet reproducible) scripts for image analysis. This has proven to be a workable yet flawed method for processing our large amount of images. The main issue we ran into is because MegaDetector (which is described in more detail in the next section), produced a large amount of false positives in our region. This may have resulted from MegaDetector not being calibrated to highly seasonal landscapes, resulting in very slow processing of images through LabelStudio. This made data processing in LabelStudio slower and more cumbersome than we initially anticipated, as LabelStudio is not well equipped for group tagging of false positives. However, in the absence of a better alternative, we persevered in using the platform and attempted to circumvent these issues by trying to reduce the number of false positives in the MegaDetector output. We attempted to find patterns in the false positive rates and to train a separate classifier but we were unsuccessful. As we continued, it slowly became clear that LabelStudio could not scale appropriately to multi-year data. Concurrently, the WildTrax platform improved significantly. WildTrax now supports MegaDetector and image filtering based on score; something we had been conducting manually through our LabelStudio protocol. It started being able to display the bounding boxes created by MegaDetector and exposed these results in its API, allowing for further processing. Compared to 2022, it now has much better batch tagging and species verification abilities. Currently, it is still missing the ability to assign specific tags to bounding boxes within the app. As of March 2024, we are actively shifting our workflow to WildTrax, reanalyzing all previous work conducted in LabelStudio. For transparency, we outline both LabelStudio and WildTrax protocols below.

### Previous workflow (LabelStudio)

#### Image Detection

For the LabelStudio workflow, the images were first processed through the animal detection model MegaDetector (@beery2019efficient), version 5.0, using a Lenovo X1 laptop with an NVIDIA GeForce GTX 1650 Max-Q graphics card, which took about 2 weeks of quasi-continuous processing to complete.

```{r echo=FALSE}
#| label: basics-stats-1

# Select and distinct down to image-based infos
dets_images <- dets |> 
  dplyr::select(image_id, max_confidence, isempty) |> 
  dplyr::distinct()

# Breakdown of which images triggered a MD detection
dets_tb <- dets_images$isempty |> table()
dets_tb_sum <- sum(dets_tb)
dets_tb_percent <- dets_tb/dets_tb_sum*100

dets_tb_fmt <- prettyNum(dets_tb, big.mark = ",")

# Check total number of images
stopifnot(dets_tb_sum == length(unique(dets$image_id)))
```

MegaDetector is an AI model that predicts areas of an image that are likely to contain an animal, a person, or a vehicle, producing rectangular bounding boxes that cover such areas. It processes each image and produces “detections” at a variable confidence score (between 0 and 1, excluding 0 as this would mean the model predicts nothing on the image). About **`r dets_tb_percent[1]` %** of the images, (**`r dets_tb_fmt[1]`** images), were found by the model to possibly contain either an animal, person, or vehicle. The model did not produce detections for **`r dets_tb_fmt[2]`** images, (**`r dets_tb_percent[2]` %**) of the total.

```{r echo=FALSE}
#| label: basics-stats-2

dets_0 <- dets |> dplyr::filter(confidence > 0)
dets_0.1 <- dets |> dplyr::filter(confidence > 0.1)

# Breakdown of detections by categories (animal, person, vehicles)
dets_tb_cat_unfil <- dets_0 |> dplyr::pull(category_id) |> table()
dets_tb_cat_unfil_sum <- sum(dets_tb_cat_unfil)
dets_tb_cat_unfil_percent <- dets_tb_cat_unfil/dets_tb_cat_unfil_sum*100

dets_tb_cat_unfil_fmt <- prettyNum(dets_tb_cat_unfil, big.mark = ",")
dets_tb_cat_unfil_sum_fmt <- prettyNum(dets_tb_cat_unfil_sum, big.mark = ",")

# Breakdown of detections above 0.1 by categories (animal, persons, vehicles)
dets_tb_cat <- dets_0.1 |> dplyr::pull(category_id) |> table()
dets_tb_cat_sum <- sum(dets_tb_cat)
dets_tb_cat_percent <- dets_tb_cat/sum(dets_tb_cat)*100

dets_tb_cat_fmt <- prettyNum(dets_tb_cat, big.mark = ",")
dets_tb_cat_sum_fmt <- prettyNum(dets_tb_cat_sum, big.mark = ",")
```

Because the model can produce more than one detection per image, this resulted in a total of **`r dets_tb_cat_unfil_sum_fmt`** detections. The totals for each category are: **`r dets_tb_cat_unfil_fmt[1]`** animals (or **`r dets_tb_cat_unfil_percent[1]` %**); **`r dets_tb_cat_unfil_fmt[2]`** persons (or **`r dets_tb_cat_unfil_percent[2]` %**); and **`r dets_tb_cat_unfil_fmt[3]`** vehicles (or **`r dets_tb_cat_unfil_percent[3]` %**).

The overwhelming majority of the detections were of very low confidence (85% of detections are below a score of 0.1). This explains the high number of "person" and "vehicle" detections which are of course unrealistic. It is recommended to filter low probability detections and cut-off values of 0.1 or 0.2 were suggested in the [MegaDetector documentation](https://github.com/microsoft/CameraTraps) at the time the model was run.

As the NSERC Alliance Wildlife Working Group, we decided to use a cut-off value of 0.1 for detection confidence (i.e. discarding all detections with confidence below 0.1), leaving us with a total of **`r dets_tb_cat_sum_fmt`** detections to sort through (see @fig-detections-histogram for a histogram of the detection scores, grouped by detection types). After applying this confidence threshold (decided on by the Wildlife Working Group) the total images to process for each category were: **`r dets_tb_cat_fmt[1]`** animals (**`r dets_tb_cat_percent[1]` %**); **`r dets_tb_cat_fmt[2]`** persons (**`r dets_tb_cat_percent[2]` %**); and **`r dets_tb_cat_fmt[3]`** vehicles (**`r dets_tb_cat_percent[3]` %**).

```{r echo=FALSE}
#| label: fig-detections-histogram
#| fig-cap: "Histogram of detection confidence scores produced for each bounding box predicted by MegaDetector on camera trap images collected from the Northern Ontario array between March 2022 and March 2023, showing a bimodal distribution. The colors represent the three categories of object identified by MegaDetector: animals, persons and vehicles. Note that the histogram is truncated at our selected confidence threshold of 0.1."
#| fig.width: 6
#| fig.height: 3

# Histogram of confidence scores above 0.1
dets_0.1 |> 
  dplyr::mutate(category_id = as.character(category_id)) |> 
  dplyr::mutate(Type = dplyr::case_match(category_id,
                                         "0" ~ "Empty",
                                         "1" ~ "Animal",
                                         "2" ~ "Person",
                                         "3" ~ "Vehicle")) |> 
  ggplot2::ggplot() +
  ggplot2::theme_bw() +
  ggplot2::geom_histogram(ggplot2::aes(x = confidence, group = Type, fill = Type),
                          bins = 100) +
  ggplot2::scale_fill_viridis_d() +
  ggplot2::xlab("Confidence Score") +
  ggplot2::ylab("Count") +
  ggplot2::geom_vline(ggplot2::aes(xintercept = 0.1), lty = 2, col = "firebrick")
```

Finally, it is important to note that not only pictures with detections were uploaded to the LabelStudio platform. Camera traps produce images in bursts (in our case, 5 images per trigger). It is possible for MegaDetector to miss a detection within a burst, or to produce variable confidence scores within that burst, which could lead to missing true detections if not all pictures from a given burst are included. Therefore, if any image from a burst exceeded the selected MegaDetector threshold, we included it in our analysis.

#### Tagging platform selection

We had the following criteria in mind when selecting an adequate tool to tag our images and bounding boxes:

-   *Deployable online*. This was to allow for multiple people to help process the images.
-   *Flexible tagging interface*. Few platforms allow for a full customization of the tagging interface, down to the smallest details.
-   *Ability to tag multiple bounding boxes per image*. Most platforms only allow for tagging the entire image, and not for portions of it (e.g. bounding boxes of detections produced by MegaDetector). This is necessary for when we crop our images to train our neural network model.
-   *Optionally, open source,* to allow for maximum reproducibility of the process.
-   *Ideally,* stored on a cloud system hosted in Canada.

To guide our choice, we used these criteria to informally compare 51 different platforms and tools currently available for processing and managing camera trap images (a list of the tools compared is given in Appendix III). We first looked at platforms traditionally used for tagging camera trap images, but found them often lacking in two main aspects: flexibility of the tagging interface, and ability to manipulate bounding box information.

These two features are often better addressed by tools not traditionally used for tagging camera trap images but used by the larger community of machine learning practitioners. Indeed, machine learning datasets can be very diverse and often more detailed than camera trap datasets, requiring processing tools with more detailed features (for example, see @Schneider2021). Tagging is a common step in machine learning research and industry and is often referred to as labeling.

We found two platforms that matched all of our criteria: LabelStudio (@LabelStudio) and CVAT (@boris_sekachev_2020_4009388). After testing both platforms, LabelStudio was chosen because the data format it uses is more easily compatible with MegaDetector's COCO format output, making it easier to set up. We deployed the Community edition LabelStudio (version 1.6.0rc5) on an Ubuntu virtual machine hosted by Compute Canada, using an Apache server and an NGINX reverse proxy, which is a standard approach for custom hosting of websites.

### Updated workflow (WildTrax)

#### Image renaming and organization

WildTrax keeps track of image batches using "location folders". We created our location folders by combining images within the same retrieval, site, and location IDs. The standard format we used for location folders follows this convention: 

- "\textcolor{red}{retrievalID}\_\textcolor{blue}{siteID}\_\textcolor{green}{locationID}\_\textcolor{purple}{cameraID}

Giving, for example, a folder with the name:

- "\textcolor{red}{TC1}\_\textcolor{blue}{P028}\_\textcolor{green}{P028-1}\_\textcolor{purple}{WLU-10}"

For locations in which images were previously sorted into "wildlife" and "nonwildlife", we divided each location folder into "_W" and "_NW" variants and uploaded as such to WildTrax.

To reduce the impact of false positives on our workflow, we resorted to adding a pre-processing step in our workflow in which most of the images were manually sorted into "wildlife" and "nonwildlife" folders inside each of the camera subfolders. In addition, because WildTrax does not track the upload path of an image, and instead tracks the unique name of each file, we are required to change the way we encode unique images. Therefore, we developed a simple and very efficient script in the Julia programming language to parallelize the renaming of each file in a standardized manner. We had originally refrained from using this method of uniquely identifying files and instead relied on tracking the unique path of each file as we wanted to reduce the impact on our storage capacity (as it essentially required doubling the required storage space). 

The standard format we used for file renaming follows this convention: 

- "\textcolor{red}{retrievalID}\_\textcolor{blue}{siteID}\_\textcolor{green}{locationID}\_\textcolor{purple}{cameraID}\_\textcolor{orange}{subfolderID}\_\textcolor{PineGreen}{fileID}\_\textcolor{cyan}{dateTime}.JPG", 

Giving, for example, a file with the name: 

- "\textcolor{red}{TC1}\_\textcolor{blue}{P028}\_\textcolor{green}{P028-1}\_\textcolor{purple}{WLU-10}\_\textcolor{orange}{100}\_\textcolor{PineGreen}{0001}\_\textcolor{cyan}{2022\_03\_18\_09\_44\_10}.JPG". 

We also added a "W" if the image had been sorted into a "wildlife" folder, ie: "TC2_INTC02_INTC02-2_CWS-ON46_*W*_0099_2022_12_17_13_41_00.JPG".

#### Current tagging workflow and postprocessing

As a first step in testing the WildTrax workflow, we are uploading the folders from the first retrieval that showed the highest rates of false positives, retagging and evaluating the level of overlap with the LabelStudio workflow and also allowing us to complete these folders. As a second step, we are uploading the folders from the very last retrieval, to speed up the tagging progress. What remains to be determined is how we will properly assign tags to bounding boxes. We are currently considering two main avenues: 1. re-purposing the previous LabelStudio workflow for bounding box assignment, and 2. using a different, nonnetworked platform.

## Species Classifier

We made a first attempt at developing a species classifier using the tagged data from all the images retrieved in 2022. Following the work of @beery2019efficient, we chose to use the bounding box of each verified detection instead of the entire image as the basis for our data. This resulted in 23730 cropped bounding boxes to be used for training and validation. We used the Pytorch framework (@Paszke_PyTorch_An_Imperative_2019), a widely used state-of-the-art library for developing neural network models in Python. As a first attempt, we settled on a model of the ResNet 100 architecture (@he2015deep), as it is widely used basal architecture for training species classifiers (for example, see @Khan2020, @Ukwuoma2022, and @BintaIslam2023), although the field is progressing and innovating on new methods and architectures rapidly (for example, see @Elhamod2021 or @radford2021learning). 

We chose to train the model for 100 epochs with a batch size of 32 as an arbitrary starting point for training parameters. We encountered issues in overfitting, which we tackled by modifying the last layer of our network to include a regularization (i.e. dropout) layer (as recommended by @Srivastava2014). We tested the species classifier training via both transfer learning (frozen weights of a pre-trained ResNet model) and classic learning (unfrozen weights, learning from the data only), as this is currently a comparison of interest in the field of AI applied to camera trap data (@schneider2018deep, @BintaIslam2023).

## Tagging progress - LabelStudio platform

```{r echo=FALSE}
#| label: basics-stats-3

# Number of uploaded images
uploaded <- dets |> dplyr::filter(confidence > 0.1) |> dplyr::pull(image_id) |> 
  unique() |> length()

uploaded_fmt <- prettyNum(uploaded, big.mark = ",")

# Number of images viewed
viewed <- length(unique(anns_wide$image_id))
viewed_fmt <- prettyNum(viewed, big.mark = ",")

# Percentage of image viewed (in the total)
viewed_pct <- viewed/length(unique(dets$image_id))*100

# Percentage of image viewed from what was uploaded to the platform
viewed_pct_up <- length(unique(anns_wide$image_id))/uploaded*100

# Number of processed annotations
processed <- nrow(anns_wide_noempty)
processed_fmt <- prettyNum(processed, big.mark = ",")

# Percentage of animal annotations processed
processed_pct <- processed/dets_tb_cat[1]*100

# Number of nonempty images viewed
processed_img <- length(unique(anns_wide_noempty$image_id))
processed_img_fmt <- prettyNum(processed_img, big.mark = ",")

# Percentage of nonempty images viewed (in the total)
processed_img_pct <- processed_img/length(unique(dets$image_id))*100

# Percentage of nonempty images viewed (in the detected images > 0.1)
processed_img_up_pct <- processed_img/uploaded*100
```

Out of the **`r dets_tb_cat_fmt[1]`** animal detections, **`r processed_fmt`** have been processed to date using the LabelStudio platform (or **`r processed_pct` %**) for a total of **`r processed_img_fmt`** images (i.e. **`r processed_img_pct` %** of the total amount of images, or **`r processed_img_up_pct` %** of the total amount of images uploaded to the platform).

As stated above, all images of a given burst are viewed if at least one image in the burst contains at least one detection with confidence above the MegaDetector confidence threshold. This includes potentially empty images. If we add the empty images viewed as part of a burst, it brings down the total amount of viewed images to **`r viewed_fmt`** images, (i.e. **`r viewed_pct` %** of the total amount of images, or **`r viewed_pct_up` %** of the total amount of images uploaded to the platform).

```{r echo=FALSE}
#| label: manual-annotations

# Deleted predictions
dels <- anns_wide_noempty |> dplyr::filter(is.na(species))
dels_nb <- nrow(dels)
dels_nb_fmt <- prettyNum(dels_nb, big.mark = ",")
dels_nb_pct <- dels_nb/nrow(anns_wide_noempty)*100

# Should be no manuals here
stopifnot((anns_wide_noempty |> dplyr::filter(is.na(species)) 
           |> dplyr::filter(manual)) == 0)

# Actual annotations
true_anns <- anns_wide_noempty |> dplyr::filter(!is.na(species))
true_anns_nb <- nrow(true_anns)
# true_anns_nb_fmt <- prettyNum(true_anns_nb, big.mark = ",")
# true_anns_nb_pct <- true_anns_nb/processed*100

# Non manuals 
non_mans <- anns_wide_noempty |> dplyr::filter(!is.na(species)) |> 
  dplyr::filter(!manual)
non_mans_nb <- nrow(non_mans)
non_mans_nb_fmt <- prettyNum(non_mans_nb, big.mark = ",")
non_mans_nb_pct <- non_mans_nb/nrow(anns_wide_noempty)*100

# Mans
mans <- anns_wide_noempty |> dplyr::filter(!is.na(species)) |> 
  dplyr::filter(manual)
mans_nb <- nrow(mans)
mans_nb_fmt <- prettyNum(mans_nb, big.mark = ",")
mans_nb_pct <- mans_nb/nrow(anns_wide_noempty)*100

# adjusted
orig_tb <- anns_wide_noempty$origin |> table()
orig_tb_pct <- orig_tb/sum(orig_tb)*100
orig_tb_fmt <- prettyNum(orig_tb, big.mark = ",")
```

Out of the **`r processed_fmt`** detections, **`r dels_nb_fmt`** were false positives (or **`r dels_nb_pct` %**). In addition, **`r non_mans_nb_fmt`** were true positives (or **`r non_mans_nb_pct` %**), to which **`r mans_nb_fmt`** false negatives were added (manually added bounding boxes, **`r mans_nb_pct` %**). Out of the **`r non_mans_nb_fmt`** true positives, **`r orig_tb_fmt[3]`** had to have their bounding box adjusted: the bounding box drawn by MegaDetector was at least partially inaccurate and had to be corrected. The table below summarises information on the tagging progress.

```{r echo=FALSE}
#| label: tab-summary-table

sum_tab <- tibble::tribble(
  ~Status,                ~Images,           ~Detections,               ~`False Positives`,
  "Total",                nb_image_fmt,      dets_tb_cat_unfil_sum_fmt, "N/A",
  "With detections",      dets_tb_fmt[1],    "N/A",                     "N/A",
  "Empty",                dets_tb_fmt[2],    "N/A",                     "N/A",
  "Filtered (> 0.1)",     uploaded_fmt,      dets_tb_cat_sum_fmt,       "N/A",
  "Processed",            processed_img_fmt, processed_fmt,             dels_nb_fmt
)

knitr::kable(sum_tab,
             caption = "Summary table on tagging progress. The 0.1 filter value corresponds to the MegaDetector cinfidence threshold.")
```

The relatively high rate of false positives is attributable to a combination of factors. First, the high seasonality of Ontario's far north makes it difficult to predict whether a camera that was set up in the winter won't be overgrown by vegetation in the spring. This isn't a unique problem to this project, but an issue in many ecosystems. Locations where vegetation has grown in front of the sensor, combined with windy events, increase the number of photos triggered by vegetation, rather than wildlife. MegaDetector is likely to mistake many of the vegetation pictures for animals. We are currently exploring avenues to ways to address this by identifying repeating bounding images and by using a model to sort false from true positives.

## Tagging progress - WildTrax

About 97% of all images previously tagged in LabelStudio have been uploaded to the WildTrax platform (3,192,316 images), in addition to all the images from the cameras most recently retrieved. The remaining images are still being uploaded. These images are being re-tagged at the species level, and are in various stages of progress, with about half of those folders fully tagged and verified, illustrating the superior speed of tagging on the platform. A more detailed report of this progress and tagging method will be present in the final version of this report, once the revision of the ragging workflow is finalized.

## Ordination and clustering

We summarized our existing species data in two ways:  (1) species presence/absence at each camera and, (2) relative species abundance, measured as the number of days, within each month that each species was detected at a camera site. We analyzed the presence/absence and relative abundance data using published and standardized numerical ecology community analysis methods (Legendre and Legendre 2013). 

As a first step, we combined clustering and ordination to visualize associations between species, camera sites, and land cover in the vicinity of the detection site. We used Ward clustering on the relative abundances matrix. We extracted land cover information in a 1-km radius around each camera site, using the Far North Land Cover data set from Ontario's Ministry of Natural Resources and Forestry. We computed the frequencies of land cover classes and used these as constraining variables in a redundancy analysis, a method well suited for multidimensional data.

## Reproducibility

We are committed to producing a reproducible workflow. All code necessary to reproduce this report is available at an online [repository](https://github.com/StewartWILDlab/rofcamtrap), which is currently private but will be opened upon publication. The repository makes use of the [**mdtools**](https://github.com/StewartWILDlab/mdtools) module, a Python-based command line tool for converting MegaDetector outputs into LabelStudio inputs, and LabelStudio outputs into CSV tables.

# Preliminary results

## Species detections

```{r eval=FALSE, echo=FALSE}
#| label: camera-plots
#| fig-cap: "Box plot of detection time for each species in either ecozone."
#| fig.width: 8
#| fig.height: 6

bp_data <-  anns_wide_animals_joined_mut |> 
  dplyr::left_join(camera_days, by = "camera_id") |> 
  
  dplyr::select(species, camera_id, 
                date_time, camera_first_day, ymd) |> 
  
  dplyr::group_by(species, camera_id) |> 
  dplyr::summarise(first_day = min(date_time),
                   camera_first_day = min(camera_first_day),
                   camera_events = length(unique(ymd))) |> 
  dplyr::ungroup() |> 
  
  dplyr::left_join(camera_regions, by = "camera_id") |> 
  
  dplyr::group_by(species) |> 
  dplyr::mutate(total_events = sum(camera_events),
                n_cameras = dplyr::n()) |> 
  dplyr::ungroup() |>
  
  # dplyr::filter(n_cameras > 4) |>
  dplyr::filter(n_cameras > 3) |>
  
  dplyr::mutate(diff_time = difftime(first_day, camera_first_day, units = "days"),
                ECOZONE_NA = as.factor(ECOZONE_NA)) |> 
  dplyr::filter(diff_time < 200) # For mustelid error currently TODO

bp <- bp_data |> 
  ggplot2::ggplot(ggplot2::aes(x = diff_time, 
                               y = forcats::fct_reorder(species, total_events),
                               color = ECOZONE_NA)) +
  ggplot2::geom_boxplot() +
  ggplot2::scale_x_continuous() +
  ggplot2::labs(x = "Detection time (in days)",
                y = "Species",
                color = "Ecozone") + 
  ggplot2::theme_bw()

ggplot2::ggsave(plot = bp, filename = "figures/bp.png", width = 10, height = 15)

# bp
```

```{r echo=FALSE}
pts_data <- anns_wide_animals_joined_mut |>
  dplyr::left_join(camera_days, by = "camera_id") |> 
  
  
  dplyr::select(species, camera_id, 
                date_time, camera_first_day, ymd) |> 
  
  dplyr::group_by(species, camera_id) |> 
  dplyr::summarise(first_day = min(date_time),
                   camera_first_day = min(camera_first_day),
                   camera_events = length(unique(ymd))) |> 
  dplyr::ungroup() |> 
  
  dplyr::left_join(camera_regions, by = "camera_id") |> 
  
  dplyr::group_by(species) |> 
  dplyr::mutate(total_events = sum(camera_events),
                n_cameras = dplyr::n()) |> 
  dplyr::ungroup() |>
  
  # dplyr::filter(n_cameras > 4) |>
  dplyr::filter(n_cameras > 3) |>
  
  dplyr::mutate(diff_time = difftime(first_day, camera_first_day, units = "days"),
                ECOZONE_NA = as.factor(ECOZONE_NA)) |> 
  dplyr::filter(diff_time < 200) # For mustelid error currently TODO


pts <- pts_data |> 
  ggplot2::ggplot(ggplot2::aes(x = diff_time, 
                               y = forcats::fct_reorder(species, total_events),
                               # size = camera_events, 
                               color = ECOZONE_NA)) +
  ggplot2::geom_point(ggplot2::aes(x = diff_time, 
                                   y = forcats::fct_reorder(species, total_events),
                                   size = camera_events, 
                                   color = ECOZONE_NA,
                                   group = ECOZONE_NA)) +
  ggplot2::scale_x_continuous() +
  ggplot2::labs(x = "Detection time (in days)",
                y = "Species",
                color = "Ecozone",
                size = "Day counts") + 
  ggplot2::theme_bw()

ggplot2::ggsave(plot = pts, filename = "figures/pts.png", width = 10, height = 15)
```

To better visualize patterns in species detection, we computed the detection time (number of days from the start of a camera deployment until the species is detected at that camera) for each species in either ecozone, for data from cameras in the field between March 2022 and March 2023. The detection time for species is highly variable, with a mean of `r pts_data$diff_time |> mean() |> as.numeric()` days and a standard deviation of `r pts_data$diff_time |> sd() |> as.numeric()` days.

We also produced a camera deployment graph, which is too large to be included in the main text but can be found in Appendix IV. The graph shows the deployment period for all cameras (with camera ID on the y-axis, date on the x-axis, and site ID as graph labels). The graphs show deployment and retrieval dates as black dots and also include days with at least one animal detection as red dots.

```{r echo=FALSE}
#| label: camera-plots-2
#| fig-cap: "Bubble plot of detection time (number of days from the start of a camera deployment until the species is detection at that camera) for each species in either ecozone, for data between March 2022 and March 2023."
#| fig.width: 8
#| fig.height: 6
pts
```

## Wildlife community composition

We obtained repeat detections of at least 31 mammal and bird species across 193 sites and 5-7 months of deployment. The most common species detected (detected more than 50 days in the season) are listed in Table 2 below (the full table is available in Appendix I). Note the large amount detections in the “bird” and “duck” species categories which could be further distinguished into specific species; eight of the top 17 detected species categories were avian.

```{r echo=FALSE}
#| label: tab-species-table

sp_tab <- dplyr::rename(.data = events_tb_cam |> colSums() |> 
                          sort(decreasing = TRUE) |> as.data.frame(), 
                        days_detected = `sort(colSums(events_tb_cam), decreasing = TRUE)`) |> 
  tibble::rownames_to_column("species")

sp_tab |>
  dplyr::left_join(sci_names_processed, by = "species") |> 
  dplyr::relocate(Scientific_name, .after = species) |> 
  dplyr::filter(days_detected >= 50) |>
  gt::gt() |>
  gt::tab_header(
    title = "Table 2: Relative abundances for species with more than 50 detection days, computed in number of days detected in the season, for data between March 2022 and March 2023."
  ) |>
  gt::fmt_number(
    decimals = 0,
    columns = days_detected,
    suffixing = TRUE
  ) |> 
  gt::cols_label(
    days_detected = "Days",
    species = "Species",
    Scientific_name = "Scientific name"
  ) |> 
  gt::tab_style(style = list(
      gt::cell_text(style = "italic")),
    locations = gt::cells_body(columns = Scientific_name)) |> 
  gt::opt_stylize(style = 6, color = "gray")

```

As for the spatial distribution of species relative abundances, @fig-map-pies shows relative species abundances across the landscape, for species detected more than 50 days in the season (the distribution of detection days, has a mean of `r sp_tab$days_detected |> mean()` days and a standard deviation of `r `sp_tab$days_detected |> sd()`). Note that for this figure, the detections have been grouped by plot number for better visibility, with a plot containing up to 4 cameras (see deployment methods). There are no clear spatial patterns, but some generalities can be noted: Moose detections are predominant in the northwestern and southeastern zones of the sampling array, while caribou and sandhill crane detections share most of the detections for the central part of the array.

```{r echo=FALSE}
#| label: fig-map-pies
#| fig-cap: "Pie chart map of relative species abundances across our study area, for data between March 2022 and March 2023. Detections have been aggregted by plot for better visibility, with a plot containing up to 4 cameras. Only species detected more than 50 days in the season are shown for clarity."
#| fig.width: 8
#| fig.height: 6

cams_sf_with_props <- cams_sf_coords |> 
  sf::st_drop_geometry() |> 
  dplyr::select(plot_id,X,Y) |> 
  dplyr::group_by(plot_id) |> 
  dplyr::mutate(X = mean(X), Y = mean(Y)) |>
  dplyr::distinct() |> 
  dplyr::ungroup() |> 
  dplyr::left_join(events_tb_filt_with_col, by = "plot_id")

regions_labels_bis <- zones_filtered |> 
  sf::st_drop_geometry() |> 
  dplyr::bind_cols(data.frame(x = c(-85, -88),
                              y = c(52, 52.6)))

base_map +
  ggplot2::coord_equal(xlim = c(-88.5, -84),
                       ylim = c(50, 54)) +
  scatterpie::geom_scatterpie(data = cams_sf_with_props,
                              ggplot2::aes(x = X, y = Y, group = plot_id),
                              cols = names(cams_sf_with_props)[4:(4 + ncol(cams_sf_with_props) - 4)],
                              sorted_by_radius = TRUE,
                              legend_name = "Species",
                              pie_scale = 1.5) +
  ggplot2::labs(x = "Lat", y = "Long") +
    ggplot2::geom_label(ggplot2::aes(x = x, y = y, label = ECOZONE_NA),inherit.aes = FALSE,
                      size = 2.5, data = regions_labels_bis)
```

## Ordination - presence-absence

To start exploring general species-habitat associations we completed a redundancy analysis (RDA, using the `vegan` R package, @vegan). RDAs allow to study of the relationships between two matrices of data and is often used in community ecology.

```{r echo=FALSE}
#| label: rda-pa
#| echo: false

anns_wide_animals_table_tb_ch <- vegan::decostand(anns_wide_animals_table_tb, 
                                                  "hellinger")
pa_rda <- vegan::rda(anns_wide_animals_table_tb ~ .,
                     extr_table_sub_tb)
```

We used the Ontario Far North Land Cover dataset to produce the proportions of land covers at each site (@hogg2014far, a full legend of the land cover abbreviations are to be found in Appendix II). @fig-rda-pa shows the triplot for the RDA on presence-absence data, which yielded a low $R^2$ of `r round(as.numeric(vegan::RsquareAdj(pa_rda)[2]), 2)`, but which is still relevant for exploring species associations. Land cover might not be able to explain community structure, but it can still show which species are found together and around what kind of land cover class. Here, as expected, we see that caribou and sandhill cranes are associated with open and treed wetlands such as open and treed bogs and fens (OBOG, OFEN, TrBOG, and TrFEN) while black bears and moose are found together in more forested areas of mixed trees (MixTRE), sparse Trees (SpTRE) and coniferous trees (ConTRE). Canada geese are associated with water (WAT) while smaller species like snowshoe hares and martens score away from the larger mammals in a mix of habitats, including swamps (SWA).
```{r echo=FALSE}
#| label: fig-rda-pa
#| fig-cap: "Triplot of wildlife presence-absence data from Northern Ontario camera trapping images collected between March 2022 and March 2023 reveals that different species associate with different Far North land cover classes. For example, caribou and cranes are more likely to be detected near open and treed wetlands (OBOG, OFEN, TrBOG, and TrFEN) while black bear and moose near forested areas (MixTRE, SpTRE, ConTRE)." 
#| fig.width: 8
#| fig.height: 6

custom_rda_plot(pa_rda)
```

## Ordination - relative abundances

```{r echo=FALSE}
#| label: rda-events
#| fig-cap: "Triplot of RDA on relative abundance data"

events_tb_cam_hell <- vegan::decostand(events_tb_cam, "hellinger")

events_rda <- vegan::rda(events_tb_cam_hell ~ .,
                         extr_table_sub_tb)
```

To continue exploring abundance-habitat relationships as a proxy for species' relative habitat use, we also conducted an RDA using relative abundances instead of presence-absence. @fig-rda-events shows the triplot for the RDA on relative abundance data. This analysis yielded a slightly larger $R^2$ of `r round(as.numeric(vegan::RsquareAdj(events_rda)[2]), 2)`, than the analysis in @fig-rda-pa presenting presence-absence data, but reduces the axis scores of the rarer species. We observe similar patterns for caribou, cranes, moose, black bears, and marten. 
```{r echo=FALSE}
#| label: fig-rda-events
#| fig-cap: "Triplot of wildlife relative abundance data from Northern Ontario camera trapping images collected between March 2022 and March 2023, showing similar patterns as the triplot using presence-absence data."
#| fig.width: 8
#| fig.height: 6

events_tb_cam_hell <- vegan::decostand(events_tb_cam, "hellinger")

events_rda <- vegan::rda(events_tb_cam_hell ~ .,
                         extr_table_sub_tb)

# vegan::RsquareAdj(events_rda)
custom_rda_plot(events_rda, sp_scale = 1.5, scale_x_sp = 0.4, scale_y_sp = 0.3,
                thres_plot = 0.2)
```

## Clustering

```{r echo=FALSE}
#| label: cluster-base
#| echo: false

events_gower <- cluster::daisy(events_tb_cam_hell, "gower")
clust <- hclust(events_gower, method = "ward.D2")
# plot(clust)
# km <- vegan::cascadeKM(events_gower, 2, 5)
```

To start inferring patterns from the above ordination analyses we used a clustering algorithm on the relative abundance data presented in @fig-rda-events. @fig-rda-events-clust shows the triplot for the RDA on relative abundance data, with Ward clusters as ellipses (@Legendre2012). We divided our ordination plot into 2 clusters along the first RDA axis. One cluster for sites primarily found with caribou and cranes, and a second cluster for the rest of the cameras and species. This clustering generally reflects the spatial patterns in species abundance that we observed in @fig-map-pies.

<!-- We used the cascading K-means function `cascadeKM` in the `vegan` package (@vegan) to determine the number of relevant clusters to show. We used the Calinski criteria (@Legendre2012) to decide on the appropriate number of cluster. This criteria is // We found that the optimal number of clusters was 2. -->

```{r echo=FALSE}
#| label: fig-rda-events-clust
#| fig-cap: "Triplot of wildlife relative abundance data from Northern Ontario camera trapping images collected between March 2022 and March 2023, showing similar patterns than the triplot using presence-absence data. Two clusters determined using Ward clustering are shown as ellipses."
#| fig.width: 8
#| fig.height: 6

clust_grp <- cutree(clust, k = 2)
custom_rda_plot(events_rda, sp_scale = 1.5, scale_x_sp = 0.4, scale_y_sp = 0.3,
                thres_plot = 0.2716, clust = clust_grp)
```

These clusters can be visualized on a map, which @fig-map-clust shows. There is no apparent discernible spatial pattern.

```{r echo=FALSE}
#| label: fig-map-clust
#| fig-cap: "Map of camera sites clustered by Ward Clustering for data collected between March 2022 and March 2023."
#| fig.width: 8
#| fig.height: 6

grps_df <- data.frame(camera_id = names(clust_grp),
                      grp = as.factor(clust_grp))

cams_sf_clust <- cams_sf_coords |> 
  # dplyr::filter(camera_id %in% names(clust_grp)) |> 
  dplyr::left_join(grps_df, by = "camera_id") |> 
  dplyr::filter(!(is.na(grp)))

base_cut <- base_map +
  ggplot2::coord_map(xlim = c(-89, -84),
                     ylim = c(50, 54.5))

base_cut +
  ggplot2::geom_point(data = cams_sf_clust, cex = 1, shape = 21, col = "black",
                      ggplot2::aes(x = X, y = Y, fill = grp), 
                      inherit.aes = FALSE) +
  ggplot2::labs(fill = "Group")
```

## Spatial clustering

```{r echo=FALSE}
#| label: clust-geo

# Spatially constrained clust

events_gower_mod <- events_gower
class(events_gower_mod) <- "dist"

tree <- ClustGeo::hclustgeo(events_gower_mod)
# plot(tree)
# rect.hclust(tree ,k = 5, border = c(4,5,3,2,1))
# legend("topright", legend = paste("cluster",1:5), 
#        fill=1:5,bty= "n", border = "white")

dist_geo <- cams_sf_clust |> 
  dplyr::arrange(camera_id) |> 
  sf::st_distance() |> 
  as.dist()

# range.alpha <- seq(0,0.2,0.01)
K <- 4

# cr <- ClustGeo::choicealpha(events_gower_mod, dist_geo, range.alpha, 
#                             K, graph = F)
# plot(cr)

clust_geo <- ClustGeo::hclustgeo(events_gower_mod,
                                 dist_geo, alpha = 0.1)
clust_geo_grp <- cutree(clust_geo, k = K)

grps_geo_df <- data.frame(camera_id = names(clust_geo_grp),
                          grp = as.factor(clust_geo_grp))

cams_sf_geo_clust <- cams_sf_coords |> 
  # dplyr::filter(camera_id %in% names(clust_grp)) |> 
  dplyr::left_join(grps_geo_df, by = "camera_id") |> 
  dplyr::filter(!(is.na(grp)))

# plot(clust_geo)
```

Spatial clustering allows to add Euclidean distance between sites as a soft constraint in the clustering algorithm. The extent to which these distances influence the result in balance with the community data can be adjusted, by setting the strength of the mixing parameter alpha. The `ClustGeo` package (@Chavent2018) allows testing different values of that parameter to determine optimal mixing of the clustering forces, with the function `choicealpha`.

We found an alpha level of 0.1 to be optimal for this data set and performed mixed clustering using the `hclustgeo` function. @fig-map-clust-geo shows the resulting map of geographically constrained clustered camera sites.

```{r echo=FALSE}
#| label: fig-map-clust-geo
#| fig-cap: "Map of geographically constrained clustered camera sites, for data collected between March 2022 and March 2023, using geographically constrained clustering with a mixing parameter of 0.1."
#| fig.width: 8
#| fig.height: 6

base_cut +
  ggplot2::geom_point(data = cams_sf_geo_clust, cex = 1, shape = 21, col = "black",
                      ggplot2::aes(x = X, y = Y, fill = grp), 
                      inherit.aes = FALSE) +
  ggplot2::labs(fill = "Group")
```

## Species heatmaps and bubbleplots

```{r echo=FALSE}
#| label: join-sf
#| echo: false

# cams_sf_sp <- anns_wide_animals_joined_mut |> 
#   dplyr::left_join(cams_sf_coords, by = dplyr::join_by(camera_id, plot_id)) |> 
#   dplyr::filter(species %in% colnames(events[,-1]))

cams_sf_sp <- events_cam_days |> 
  dplyr::left_join(cams_sf_coords, by = dplyr::join_by(camera_id)) # |> 
  # dplyr::filter(species %in% colnames(events_tb_filt_with_col[,-1]))

cams_sf_sp_sum <- cams_sf_sp |> 
  dplyr::group_by(camera_id, X, Y, geometry, species, month) |> 
  dplyr::summarise(count = dplyr::n()) |> 
  dplyr::ungroup() # |> 
# dplyr::group_by(camera_id, X, Y, species) |> 
# dplyr::mutate(count=count/sum(count)) |> 
# dplyr::ungroup() 
# stopifnot(dim(cams_sf_sp)[1]==dim(anns_wide_animals_joined_mut)[1])
```

```{r eval=do_eval, echo=FALSE}
#| label: bubbleplots

# cams_sf_sp_sum |>
#   # dplyr::filter(species == "Caribou") |>
#   ggplot2::ggplot() +
#   ggplot2::geom_point(ggplot2::aes(x=X, y=Y, size=count, col =species)) +
#   ggplot2::facet_wrap(~month) +
#   ggplot2::coord_equal(xlim = c(-89, -84),
#                        ylim = c(50, 55))

plot_list <- list()

for (sp in unique(cams_sf_sp_sum$species)) {
  
  dat <- sf::st_as_sf(cams_sf_sp_sum) |> 
    dplyr::filter(species == sp)
  
  gg <- base_cut +
    ggplot2::geom_point(data = dat, col = "black",
                        ggplot2::aes(x = X, y = Y, size = count), 
                        inherit.aes = FALSE) +
    ggplot2::facet_wrap(~month) +
    ggplot2::labs(size = "Days \ndetected",
                  title = sp)
  if (sp %in% c("Caribou", "Sandhill crane", "Moose", "Wolf")) {
    plot_list[[sp]] <- gg
  }
  
  ggplot2::ggsave(gg, filename = here::here("figures",paste0(sp,"_bplot.png")))
}

saveRDS(plot_list, here::here("data", "objects", "bbplot_list.rds"))
```

We use a series of bubble plots to visualize species' relative abundance through space and time. The following plots are for caribou, sandhill crane, moose, and wolf. They represent days detected at each camera site per month.

```{r echo=FALSE}
#| label: bubbleplots-Caribou
#| fig.width: 8
plot_list <- readRDS(here::here("data", "objects", "bbplot_list.rds"))
plot_list["Caribou"]
```
```{r echo=FALSE}
#| label: bubbleplots-Sandhill
#| fig.width: 8
plot_list["Sandhill crane"]
```
```{r echo=FALSE}
#| label: bubbleplots-Moose
#| fig.width: 8
plot_list["Moose"]
```
```{r echo=FALSE}
#| label: bubbleplots-Wolf
#| fig.width: 8
plot_list["Wolf"]
```

## Take Home messages

We can derive a few take-home messages from our initial exploration of the data:

-   Using an established object detection model for camera trap images (i.e., MegaDetector) allowed us to filter out large quantities of empty images, but performed somewhat poorly in certain conditions (ie, windy sites with drastic changes in vegetation).

-   Associative patterns: species associate with land cover classes that correspond to expected habitat, such as wetlands for caribou and cranes, or forested areas for moose.

-   Spatial clustering and considerations: latitudinal relationships may dominate this data set and drive species-habitat associations.

-   Species classifier: we obtained better accuracy (94%) with classic learning than transfer learning (91%). We are still exploring avenues for improvement, but the small sample size is likely the main limiting factor.

# Next steps

Over the coming months, we will continue to process the retrieved data and anticipate adding to this data set as images from the 2024 deployments are retrieved. The next steps for this project are to complete the tagging process and refine to the species level all detections currently categorized as "unknown", "bird", "Duck", and "mustelid". Dr. Stewart will host an undergraduate research student in the summer of 2024 to help with the "mustelid" category if funding is approved through the NSERC USRA program. We will also further explore the spatial community composition patterns at each camera site. Given a high enough data density for certain species future possibilities for this project include modeling species occupancy (e.g. @MacKenzie2002) and spatially explicit density estimation using spatially explicit capture-recapture models (e.g. @Chandler2013). We will perfect the new tagging workflow and continue testing classifier methods.

# Acknowledgements

We thank our research assistant, Meghna Pal, as well as our student volunteers at Wilfrid Laurier University for their help with camera tagging: Teea Curlew, Bridget Matthews, Rafay Siraj, and Dietrich Westberg. We also thank members of the WILDlab for feedback throughout this process: Claudia Haas, Eric Jolin, Charlotte Rentmeister, and Sheyda Zand. This work was supported by an ECCC G&C, an NSERC Canada Research Chair (Tier II), an NSERC Alliance grant, and Canadian Foundation for Innovation funds to Dr. Frances Stewart.

{{< pagebreak >}}

# References

<!-- The following line ensures the references appear here for the MS Word or HTML output files, rather than right at the end of the document (this will not work for PDF files): -->

::: {#refs}
:::

{{< pagebreak >}}

# Appendices

## Appendix I: Species table

```{r echo=FALSE}
#| label: tab-species-table-full

sp_tab |>
  gt::gt() |>
  gt::tab_header(
    title = "Table 3: Relative abundances for species for all species, computed in number of days detected in the season, for data between March 2022 and March 2023. values represent the number of days each species was detected at all available cameras",
    subtitle = "Computed in number of days detected in the season"
  ) |>
  gt::fmt_number(
    decimals = 0,
    columns = days_detected,
    suffixing = TRUE
  ) |> 
  gt::cols_label(
    days_detected = "Days",
    species = "Species",
  ) |> 
  gt::opt_stylize(style = 6, color = "gray")
```

{{< pagebreak >}}

## Appendix II: Land cover classes table

```{r echo=FALSE}
#| label: tab-landcover
table_cat|>
  gt::tab_header(
    title = "Table 4: Land cover classes abbreviations corresponding to the Ontatio Far North Land cover dataset (adapted from dataset's metadata), along with the percentage land cover within the 1km buffer areas used for the the multivariate analyses in this report."
  )
```

{{< pagebreak >}}

## Appendix III: List of camera trapping tools compared

-  Aardwolf
-  Agouti
-  AIDE
-  animl
-  animl (R)
-  CAIMAN
-  Camelot
-  Camera Base
-  Camera Trap Manager
-  CameraTrapDetectorR
-  CameraTraps
-  Camtrap DP
-  camtrapR
-  Conservation AI
-  Cos4Cloud FASTCAT
-  CPW Photo Warehouse
-  DeepFaune
-  digiKam
-  EcoAssist
-  eMammal
-  Event Finder Suite
-  IBEIS
-  IL2BB BBoxEE
-  Mapview
-  mbaza
-  MD Fast API
-  MLWIC MLWIC2 
-  PhotoSpread
-  rapidPop
-  Renamer & CamTrap
-  Snoopy
-  sparcD
-  SpeedyMouse
-  Timelapse
-  TrailCamData
-  TRAPPER
-  U-Infuse
-  ViXeN
-  Wild ID
-  WILDCAM
-  wildcam
-  Wildepod
-  WildEye TrapTagger
-  Wildlife Insights
-  Wildlife observer network
-  Wildme Wildbook
-  WildTrax
-  WTB Where's The Bear
-  zamba
-  Zooniverse

## Appendix IV: Camera Deployment History

```{r, echo=FALSE}
#| fig.width:  8
#| fig.height: 12

image_data <- anns_wide_animals_joined_mut |> 
  dplyr::select(id, source_file_with_dep,
                image_id, species:comment, camera_id,
                maker_notes_sequence:maker_notes_user_label, plot_id:ymd) |> 
  dplyr::mutate(site_id = sapply(image_id, \(x) strsplit(x, "_")[[1]][1]),
                date_time = lubridate::as_date(date_time)) |> 
  dplyr::rename(file_path = source_file_with_dep) |> 
  dplyr::filter(!is.na(date_time))

cam_dep_data <- cams_sf_coords |> 
  sf::st_drop_geometry() |> 
  dplyr::select(dplyr::ends_with("_id"), date_deployed, date_retrieved, longitude, latitude,
                deployment_group) |> 
  dplyr::mutate(date_deployed = lubridate::as_date(date_deployed),
                date_retrieved = lubridate::as_date(date_retrieved))

stopifnot(sum(table(cam_dep_data$camera_id) != 1) == 0)

cam_dep_data_with_status <- cam_dep_data |>
  dplyr::mutate(status = ifelse(date_deployed < date_retrieved, "valid", "invalid")) |>
  dplyr::mutate(status = ifelse(is.na(status), "invalid", status))

cam_dep_data_with_status_1 <- 
  cam_dep_data_with_status[1:(nrow(cam_dep_data_with_status)/2),]
cam_dep_data_with_status_2 <- 
  cam_dep_data_with_status[((nrow(cam_dep_data_with_status)/2) + 1):nrow(cam_dep_data_with_status),]

cam_dep_data_with_status_1 |>
  ggplot2::ggplot(ggplot2::aes(x = date_deployed, y = reorder(camera_id,
                                                              date_deployed,
                                                              decreasing = TRUE))) +
  ggplot2::geom_segment(ggplot2::aes(xend = date_retrieved, yend = camera_id), 
                        alpha = 0.15, linewidth = 3) +
  ggplot2::geom_point(ggplot2::aes(x = date_deployed, y = camera_id)) +
  ggplot2::geom_point(ggplot2::aes(x = date_retrieved, y = camera_id)) +

  ggplot2::geom_point(data = image_data |> 
                        dplyr::filter(camera_id %in% cam_dep_data_with_status_1$camera_id),
                      ggplot2::aes(x = date_time, y = camera_id),
                      color = "firebrick", size = 0.5) +

  ggplot2::geom_text(ggplot2::aes(x = date_deployed - 60, y = camera_id,
                                  label = site_id), size = 3) +
  # ggplot2::scale_color_discrete(type = c("#E69F00","#56B4E9")) +
  ggplot2::theme_bw() +
  ggplot2::labs(x = "Date", y = "Camera ID")
```

```{r, echo=FALSE}
#| fig.width:  8
#| fig.height: 12
cam_dep_data_with_status_2 |>
  ggplot2::ggplot(ggplot2::aes(x = date_deployed, y = reorder(camera_id,
                                                              date_deployed,
                                                              decreasing = TRUE))) +
  ggplot2::geom_segment(ggplot2::aes(xend = date_retrieved, yend = camera_id),
                        alpha = 0.15, linewidth = 3) +
  ggplot2::geom_point(ggplot2::aes(x = date_deployed, y = camera_id)) +
  ggplot2::geom_point(ggplot2::aes(x = date_retrieved, y = camera_id)) +

  ggplot2::geom_point(data = image_data |> 
                        dplyr::filter(camera_id %in% cam_dep_data_with_status_2$camera_id),
                      ggplot2::aes(x = date_time, y = camera_id),
                      color = "firebrick", size = 0.5) +

  ggplot2::geom_text(ggplot2::aes(x = date_deployed - 60, y = camera_id,
                                  label = site_id), size = 3) +
  ggplot2::theme_bw() +
  ggplot2::labs(x = "Date", y = "Camera ID")
```

{{< pagebreak >}}

## Colophon

This report was generated on `r Sys.time()` using the following computational environment and dependencies:

```{r}
#| label: colophon
#| cache: false

# which R packages and versions?
if ("devtools" %in% installed.packages()) devtools::session_info()
```

The current Git commit details are:

```{r}
#| label: git

# what commit is this file at? 
if ("git2r" %in% installed.packages() & 
    git2r::in_repository(path = ".")) {
  git2r::repository(here::here())  
}
```

<!-- 
```{r}
# -------------------------------------------------------------------------

# # Create ordered dataframe, and calculate time interval between images.
# x1 <- x |>
#   # Sometimes VNA sneaks in here
#   mutate(number_individuals = as.numeric(ifelse(number_individuals == "VNA", 1, number_individuals))) |>
#   # Amalgamate tags of same species in same image; currently broken into two separate rows
#   group_by(location, {{datetime_col}}, common_name) |>
#   mutate(number_individuals = sum(number_individuals)) |>
#   distinct(location, {{datetime_col}}, common_name, number_individuals, .keep_all = TRUE) |>
#   ungroup() |>
#   # Order the dataframe
#   arrange(project, location, {{datetime_col}}, common_name) |>
#   group_by(project, location, common_name) |>
#   # Calculate the time difference between subsequent images
#   mutate(interval = int_length({{datetime_col}} %--% lag({{datetime_col}}))) |>
#   # Is this considered a new detection?
#   mutate(new_detection = ifelse(is.na(interval) | abs(interval) >= threshold, TRUE, FALSE)) |>
#   ungroup() |>
#   # Number independent detections
#   mutate(detection = c(1, cumsum(new_detection[-1]) + 1))
# 
# # Summarise detections
# x2 <- x1 |>
#   group_by(detection, project, location, common_name, scientific_name) |>
#   summarise(start_time = min({{datetime_col}}),
#             end_time = max({{datetime_col}}),
#             total_duration_seconds = int_length(start_time %--% end_time),
#             n_images = n(),
#             avg_animals = mean(number_individuals),
#             max_animals = max(number_individuals))
```

```{r}
#| label: ecoregions-ggplot
#| echo: false

# ggplot2::ggplot(zones_filter) +
#   ggplot2::theme_bw() +
#   ggplot2::geom_sf()
```

```{r}
#| label: fig-deployment-poster

# base <- ggmap::ggmap(map) +
#   ggplot2::labs(x = "Longitude", y = "Latitude") +
#   # ggplot2::geom_polygon(data = pols, ggplot2::aes(x=X, y=Y), col = 1, fill = NA)
#   ggplot2::geom_path(data = pols, ggplot2::aes(x=X, y=Y), col = 1, linewidth=0.2)
# 
# p <- base +
#   # ggplot2::geom_label(ggplot2::aes(x=x, y=y, label=ECOREGION),inherit.aes = FALSE,
#   #                     size = 2.5, data = regions_labels) +
#   ggplot2::geom_label(ggplot2::aes(x=x, y=y, label=ECOZONE_NA),inherit.aes = FALSE,
#                       size = 2.5, data = regions_labels) +
#   ggplot2::geom_point(data = cams_sf_coords |> 
#                         dplyr::filter(is_retrieved == "Yes"), cex = 1, shape=21, 
#                       col="black", fill = "red",
#                       ggplot2::aes(x=X, y=Y), 
#                       inherit.aes = FALSE) +
#   ggplot2::labs(fill = "Camera \nRetrieved")
# p
```

```{r}
# anns_wide_animals_joined_mut |> 
#   ggplot(aes(x=date_time, y=species)) + geom_point()
# anns_wide_animals_joined_mut |> 
#   dplyr::filter(species == "Fisher") |> View()

# for_vic <- anns_wide_animals_joined_mut |> 
#   dplyr::select(id,species,date_time, camera_id) |> 
#   dplyr::left_join(cams_sf_coords |> dplyr::select(camera_id, plot_id) |> sf::st_drop_geometry(),
#                    by = "camera_id")
# tot_camera <- for_vic$camera_id |> unique() |> length()
# hit_rates <- for_vic |> dplyr::select(species, camera_id) |> 
#   dplyr::group_by(species) |> dplyr::summarise(n_cameras=length(unique(camera_id))) |> 
#   dplyr::ungroup() |> dplyr::mutate(rate = n_cameras/tot_camera*100) |> 
#   dplyr::arrange(dplyr::desc(rate))
# # det_history_two_weeks <- 
# 
# readr::write_csv(for_vic, "../data/derived_data/observations_by_camera_2023Aug15.csv")
# readr::write_csv(hit_rates, "../data/derived_data/hit_rates_by_species_2023Aug15.csv")
```

```{r}
#| label: extract-cats
#| echo: false
#| eval: false

# sum(unlist(lapply(extr_list, function(x) sum(x$cats == "NA"))))
# sum(unlist(lapply(extr_list, function(x) is.na(x$cats))))
# #  [1] "1"   "2"   "8"   "9"   "12"  "15"  "16"  "17"  "18"  "11"  "13"  "14"  "19"  "20"  "23"  "157" "247" "21"  "22"
# 
# pairs(extr_table_sub_tb, col = "dodgerblue")
# round(cor(extr_table_sub_tb), 2)
```

```{r}
# ggplot2::ggplot() +
#   scatterpie::geom_scatterpie(data = cams_sf_with_props,
#                               ggplot2::aes(x=X, y=Y, group=plot_id),
#                               cols=names(cams_sf_with_props)[4:(4+tmp_col-1)]) + 
#   ggplot2::coord_equal() #+ 
#ggplot2::theme(legend.position = "none")

# base2 <- ggplot2::ggplot() +
#   ggplot2::theme_bw()
# for (pol in 1:2){
#   base2 <- base2 + 
#     ggplot2::geom_polygon(data = dplyr::filter(pols, L2 == pol),
#                           ggplot2::aes(x=X, y=Y),
#                           col = 1, fill = "grey90")
# }
# 
# base2 +
#   ggplot2::coord_equal(xlim = c(-88.5, -84),
#                        ylim = c(50, 54)) +
#   scatterpie::geom_scatterpie(data = cams_sf_with_props,
#                               ggplot2::aes(x=X, y=Y, group=plot_id),
#                               cols=names(cams_sf_with_props)[4:(4+tmp_col-1)],
#                               sorted_by_radius = TRUE,
#                               legend_name = "Species",
#                               pie_scale = 1.5) +
#   ggplot2::labs(x="Lat", y="Long")
```

```{r}
#| label: jaccard-pcoa
#| echo: false
#| eval: false

# anns_wide_animals_table_tb_jac <- vegan::vegdist(anns_wide_animals_table_tb, 
#                                                  "jac", binary = TRUE)
# 
# pa_pcoa <- ape::pcoa(anns_wide_animals_table_tb_jac)
# 
# biplot(pa_pcoa, anns_wide_animals_table_tb)
```

As more of the camera images are labeled and our dataset grows, we can transform the above bubble plots into heat maps of species presence/absence or relative abundance. Below is an example using the existing caribou data to date. It is important to recognize that not all cameras had been deployed until June, leaving April and May. y maps with large portions of the northern study area where cameras were not yet present. 

```{r}
#| label: heatmap
#| echo: false
#| eval: false

options(backup_options)

cams_sf_sp_filt <- cams_sf_sp |> 
  dplyr::filter(species == "Caribou", month == "Jun")

cams_sf_sp_sum_filt <- cams_sf_sp_sum |> 
  dplyr::filter(species == "Caribou", month == "Jun")

mod <- mgcv::gam(count~s(X,Y), data = cams_sf_sp_sum_filt)
pred_grid <- 
  tidyr::expand_grid(X = seq(-89, -84, by = 0.1),
                     Y = seq(50, 55, by = 0.1))
preds <- pred_grid |> 
  dplyr::mutate(count = predict(mod, pred_grid))

# rast <- raster::rasterFromXYZ(preds)
# raster::plot(rast)
# rast[rast<0] <- NA
# raster::plot(rast)
```

```{r}
#| label: heatmap2
#| echo: false
#| eval: false
base_cut +
  ggplot2::geom_density2d_filled(
    data = cams_sf_sp_filt, ggplot2::aes(X, Y),
    binwidth = 0.05, alpha = 0.5)

# base_cut +
#   ggplot2::geom_tile(
#     data = dplyr::filter(preds, count>=0), ggplot2::aes(X, Y, fill=count),
#     binwidth = 0.05, alpha = 0.5, inherit.aes = FALSE) +
#   ggplot2::scale_fill_viridis_c()
```

```{r}
#| label: heatmap-facets
#| echo: false
#| eval: false

cams_sf_sp_caribou <- cams_sf_sp |> 
  dplyr::filter(species == "Caribou")

base_cut +
  ggplot2::geom_density2d_filled(
    data = cams_sf_sp_caribou, ggplot2::aes(X, Y),
    alpha = 0.5,breaks = c(seq(0.001,1, by = 0.1),5, 10, 20, 50,100), n = 20) +
  ggplot2::facet_wrap(~month) +
  ggplot2::coord_equal(xlim = c(-89, -84),
                       ylim = c(50, 55)) +
  ggplot2::theme(
    legend.position = 'none'
  )
```

```{r}
#| label: heatmaps
#| eval: false
#| echo: false

for (sp in unique(cams_sf_sp_sum$species)) {
  
  cams_sf_sp_spe <- cams_sf_sp |>
    dplyr::filter(species == sp)
  
  gg <- base_cut +
    ggplot2::geom_density2d_filled(
      data = cams_sf_sp_spe, ggplot2::aes(X, Y),
      alpha = 0.5,breaks = c(seq(0.001,1,by = 0.1),5, 10, 20, 50,100), n = 20) +
    ggplot2::facet_wrap(~month) +
    ggplot2::coord_equal(xlim = c(-89, -84),
                         ylim = c(50, 55)) +
    ggplot2::theme(
      legend.position = 'none'
    )
  ggplot2::ggsave(gg, filename = paste0("analysis/figures/",sp,"_heatmap.png"))
}
```
We can plot those geographically constrained clusters on top of the abundance-based RDA of @fig-rda-events. @fig-rda-clust-geo shows that those geographic clusters do not map well on top of the ordination, with one cluster seeming to represent a subset of a larger trend experienced in the larger part of the data.

```{r echo=FALSE}
#| label: fig-rda-clust-geo
#| fig-cap: "Triplot of wildlife relative abundance data from Northern Ontario camera trapping images collected in March through September of 2022, showing similar patterns than the triplot using presence-absence data. Two clusters determined using geographically constrained clustering with a mixing parameter of 0.1."

custom_rda_plot(events_rda, sp_scale = 1.5, clust = clust_geo_grp)
```
-->
