[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ROF Camera Trap Data Exploration",
    "section": "",
    "text": "Home page TBC."
  },
  {
    "objectID": "reports/GnC_report/paper/paper_test.html#test-2",
    "href": "reports/GnC_report/paper/paper_test.html#test-2",
    "title": "Using Camera Traps and Computer Vision to Quantify Wildlife Diversity and Co-Occurrence across Ontario’s Far North",
    "section": "Test 2",
    "text": "Test 2",
    "crumbs": [
      "GnC Report"
    ]
  },
  {
    "objectID": "reports/GnC_report/paper/paper.html",
    "href": "reports/GnC_report/paper/paper.html",
    "title": "Using Camera Traps and Computer Vision to Quantify Wildlife Diversity and Co-Occurrence across Ontario’s Far North",
    "section": "",
    "text": "Northern Ontario holds some of the world’s last intact wild places, but it is under pressure from resource extraction and global warming. Our understanding of basic biodiversity measures like species occurrence, abundance, diversity, and density for the region are still rudimental but needed to weigh ecological-economical decision tradeoffs. Traditionally, quantifying and monitoring biodiversity in northern regions required expensive and time-consuming field surveys. Today, the use of non-intrusive methods, such as arrays of 100s of camera traps, has significantly changed field practices for reliably quantifying wildlife across large and remote regions. However, these methods generate a large amount of data and require tools to speed up the identification, and processing, of image datasets.\nMachine learning methods, such as deep neural networks, are now in use to automate the process of species identification and help in that endeavor. We aim to evaluate the viability of deep neural networks for camera trap surveys currently taking place in Northern Ontario’s heterogeneous landscapes, including the Hudson Bay Lowlands and Ontario Shield ecozones, as variables like weather, seasonality, vegetation type, and species characteristics can impact the performance of these methods. The extent to which these methods perform for northern regions is not well established. There are currently no available machine learning models for identifying species from images in northern regions and developing one would be beneficial as a monitoring and modeling tool for northern boreal ecosystems.\nUsing an array of cameras deployed in an heterogeneous sampling area straddling two of Northern Ontario’s ecozones (Hudson Bay Lowlands and Ontario Shield), we first look to quantify current species diversity and co-occurrence. We’ve recently developed an open-source workflow for camera trap image identification, access, and storage, which we are using to identify images, with next steps involving the development of an image classifier.",
    "crumbs": [
      "GnC Report"
    ]
  },
  {
    "objectID": "reports/GnC_report/paper/paper.html#introduction",
    "href": "reports/GnC_report/paper/paper.html#introduction",
    "title": "Using Camera Traps and Computer Vision to Quantify Wildlife Diversity and Co-Occurrence across Ontario’s Far North",
    "section": "",
    "text": "Northern Ontario holds some of the world’s last intact wild places, but it is under pressure from resource extraction and global warming. Our understanding of basic biodiversity measures like species occurrence, abundance, diversity, and density for the region are still rudimental but needed to weigh ecological-economical decision tradeoffs. Traditionally, quantifying and monitoring biodiversity in northern regions required expensive and time-consuming field surveys. Today, the use of non-intrusive methods, such as arrays of 100s of camera traps, has significantly changed field practices for reliably quantifying wildlife across large and remote regions. However, these methods generate a large amount of data and require tools to speed up the identification, and processing, of image datasets.\nMachine learning methods, such as deep neural networks, are now in use to automate the process of species identification and help in that endeavor. We aim to evaluate the viability of deep neural networks for camera trap surveys currently taking place in Northern Ontario’s heterogeneous landscapes, including the Hudson Bay Lowlands and Ontario Shield ecozones, as variables like weather, seasonality, vegetation type, and species characteristics can impact the performance of these methods. The extent to which these methods perform for northern regions is not well established. There are currently no available machine learning models for identifying species from images in northern regions and developing one would be beneficial as a monitoring and modeling tool for northern boreal ecosystems.\nUsing an array of cameras deployed in an heterogeneous sampling area straddling two of Northern Ontario’s ecozones (Hudson Bay Lowlands and Ontario Shield), we first look to quantify current species diversity and co-occurrence. We’ve recently developed an open-source workflow for camera trap image identification, access, and storage, which we are using to identify images, with next steps involving the development of an image classifier.",
    "crumbs": [
      "GnC Report"
    ]
  },
  {
    "objectID": "reports/GnC_report/paper/paper.html#methods",
    "href": "reports/GnC_report/paper/paper.html#methods",
    "title": "Using Camera Traps and Computer Vision to Quantify Wildlife Diversity and Co-Occurrence across Ontario’s Far North",
    "section": "Methods",
    "text": "Methods\n\nCamera deployment and setup\nThe camera deployment protocol follows a spatially balanced hierarchical design that provides an even distribution of sample sites across biotic and abiotic conditions within the study area (Dam-Bates, Gansell, and Robertson (2018)). The study sites span three ecoregions (Northern Taiga, James Bay, Big Trout Lake) and two ecozones (Hudson Bay Lowlands, Ontario Shield). The network was established by CWS in 2020 and seasonally equipped in a rolling design with 500 autonomous recording units (ARUs) to monitor audible species diversity paired with site-specific on-ground vegetation imagery. In March through June of 2022 193 unbaited and unlured wildlife camera traps (Reconyx Hyper Fire II) were added to this study design. All cameras were all programmed to take bursts of 5 images (one per second) at each trigger of their motion sensor.\nBriefly, 5 ARUs are deployed per site, with 100 m between ARUs. Up to 4 cameras were paired with ARUs at each of these sites, with at least one of these cameras placed at a random ARU site and one other prioritized to focus on a game trail or open area to increase detection probability (in total, we have 43 plots with 4 cameras, 5 plots with 3 cameras and 3 plots with 2 cameras). Figure 1 shows the location of the clustered cameras and their current retrieval status. 170 cameras were retrieved in September 2022, totaling a deployment spanning 5-7 months. 5 additional cameras were only retrieved in March 2023 due to logistical and inaccessibility issues. There remains 18 cameras in the field to be retrieved in 2023.\nThe work conducted under this project focuses on quantifying and developing an open-source workflow to quantifying the wildlife species detected in those images that have been retrieved to date. \n\n\n\n\n\n\n\n\nFigure 1: Camera deployment status for the 193 cameras, 170 of which were retrieved in September 2022. Labels for the 3 ecoregions spanned by the sampling area are included.\n\n\n\n\n\n\n\nImage detection\nImages were retrieved from all cameras using a 32 GB SD card and copied onto a 10 TB hard drive (LaCie d2 Professional). The images amounted to 1.8TB of memory on this disk, for a total of 3,288,178 images. Images were stored with the following directory structure: \n\nPlot id (e.g. P128)\nSite id (e.g. P128-1)\nCamera id (e.g. CFS-10, with the first 3 letters corresponding to the group owning the camera: CFS for Canadian Forest Service, WLU for Wilfrid Laurier University)\nDCIM folder (created by the camera)\nOverflow folder (e.g. 100RECNX, 101RECNX ; each such folder contains up to 10,000 images)\n\nThe images were processed through the animal detection model MegaDetector (Beery, Morris, and Yang (2019)), version 5.0, using a Lenovo X1 laptop with a NVIDIA GeForce GTX 1650 Max-Q graphics card, which took about 2 weeks of quasi-continuous processing to complete. The model processes each image and produces “detections” (bounding boxes) at a variable confidence score.\nAt this first step into the process, no filtering was applied beyond the model predictions which classifies images as either likely to contain an object of relevance or empty. About 48.11 % of the images, (1,581,780) images, were found by the model to possibly contain either a person, animal, or vehicle. The model did not produce detections for 1,706,398 images, (51.89 %) of the total.\nBecause the model can produce more than one detection per image, this resulted in a total of 3,897,383 detections. The totals for each category are: 2,472,528 animals (or 63.44 %); 1,054,559 persons (or 27.06 %); and 370,296 vehicles (or 9.5 %).\nMegadetector is an AI model which produces confidence scores with each of its predictive detections. With no filtering, the overwhelming majority of the detections are of very low confidence (85% of detections are below a score of 0.1). This explains the high number of “person” and “vehicle” detections which are of course unrealistic. It is recommended to filter low probability detections, and cut-off values of 0.1 or 0.2 are usually suggested in the Megadetector documentation.\nUsing a cut-off value for the detection confidence of 0.1 (,i.e. discarding all detections with confidence below 0.1), this left us with a total of 499,663 detections to sort through (see Figure 2 for an histogram of the detection scores, grouped by detection types). After applying this confidence threshold (decided on by the Wildlife Working Group) the total images to process for each category are: 318,535 animals (63.75 %); 134,281 persons (26.87 %); and 46,847 vehicles (9.38 %).\n\n\n\n\n\n\n\n\nFigure 2: Histogram of detection confidence scores produced for each bounding box predicted by Megadetector, showing a bimodal distribution. The colors represent the three categories of object identified by Megadetector: animals, persons and vehicles. Note that the hsitogram is truncated at our selected confidence threshold of 0.1.\n\n\n\n\n\nFinally, it is important to note that not only pictures with detections were uploaded to the LabelStudio platform. As stated before, camera traps produce images in bursts (in our case, 5 images per trigger). It is possible for Megadetector to miss a detection within a burst, or to produce variable confidence scores within that burst, which could lead to missing true detections if not all pictures from a given burst are filtered in. Therefore, we included all the images from a given burst if any of the images in the burst include a detection with certainty above the threshold, including potentially empty images from that burst.\n\n\nTagging platform selection\nWe had the following criteria in mind when selecting an adequate tool to tag our images and bounding boxes:\n\nDeployable online. This was to allow for multiple people to help process the images.\nFlexible tagging interface. Few platforms allow for a full customization of the tagging interface, down to the smallest details.\nAbility to tag multiple bounding boxes per image. Most platforms only allow for tagging the entire image, and not for portions of it (e.g. bounding boxes of detections produced by MegaDetector). This is necessary for when we will crop our images to train our neural network model.\nOptionally, open source, to allow for maximum reproducibility of the process.\n\nTo guide our choice, we used these criteria to informally compare 51 different platforms and tools currently available for processing and managing camera trap images. We first looked at platforms traditionally used for tagging camera trap images, but found them often lacking in two main aspects: flexibility of the tagging interface, and ability to manipulate bounding box information.\nThese two features are often better addressed by tools not traditionally used for tagging camera trap images, but used by the larger community of machine learning practitioners. Indeed, machine learning datasets outside can be very diverse and often more detailed than camera trap datasets, requiring processing tools with more detailed features. Tagging is a common step in machine learning research and in industry, and if often referred to as labeling.\nWe found two platforms that matched all of our criteria: LabelStudio (Tkachenko et al. (2020-2022)) and CVAT (Sekachev et al. (2020)). After testing both platforms, LabelStudio was chosen because the data format it uses is more easily compatible with Megadetector’s COCO format output, making it easier to setup. We deployed the Community edition LabelStudio (version 1.6.0rc5) on an Ubuntu virtual machine hosted by Compute Canada, using an Apache server and a NGINX reverse proxy, which is a standard approach for custom hosting of websites.\n\n\nTagging progress\nOut of the 318,535 animal detections, 119,382 have been processed to date using our platform (or 37.48 %) for a total of 105,274 images (i.e. 3.2 % of the total amount of images, or 27.19 % of the total amount of images uploaded to the platform).\nAs stated above, all images of a given burst are viewed if at least one image in the burst contains a detections with confidence above the threshold. This include potentially empty images. If we add the empty images viewed as part of a burst, it brings the total amount to 210,489 images, (i.e. 6.4 % of the total amount of images, or 54.36 % of the total amount of images uploaded to the platform).\nOut of the 119,382 detections, 71,727 were false positives (or 60.08 %), where Megadetector identified something, but there was nothing. In addition, 45,276 were true positives (or 37.93 %), to which 2,379 false negatives were added (manually added bounding boxes, 1.99 %). Out of the 45,276 true positives, 7,234 had to have their bounding box adjusted. The table below summarises information for the tagging progress.\n\n\n\nSummary table on tagging progress\n\n\nStatus\nImages\nDetections\n\n\n\n\nTotal\n3,288,178\n3,897,383\n\n\nWith detections\n1,581,780\nN/A\n\n\nEmpty\n1,706,398\nN/A\n\n\nFiltered (&gt; 0.1)\n387,192\n499,663\n\n\nProcessed\n105,274\n119,382\n\n\n\n\n\nThe relatively high rate of false positives is attributable to a combination of factors. First, the high seasonality of Ontario’s far north makes it so that it is difficult to predict whether a camera that was set up in the winter won’t be overgrown by vegetation in the spring. Locations where vegetation has grown in front of the sensor, combined with windy events, creates a perfect storm for false cameras triggers when vegetation is moving in front of the sensors. In addition, Megadetector is a likely to mistake a certain proportion of those vegetation pictures for actual animals. We are currently exploring avenues to tackle this roadblock by identifying repeating bounding images and by using a model to sort false from true positives.\n\nReproducibility\nWe are committed to producing a reproducible workflow. All code necessary to reproduce this report is available at an online repository. The two software tools used in our workflow are also available at online repositories:\n\nmdtools: a python-based command line tool for converting Megadetector outputs into LabelStudio inputs, and LabelStudio outputs into CSV tables.\nmdscripts: a bash-based command line tool for running Megadetector and mdtools.\n\n\n\n\nAnalysis\n\nSpecies detections\nWe summarized our existing species data to date for analysis in two main ways:  (1) species presence/absence data for each camera and, (2) relative species abundance, measured as the number of days, within each month that each species was detected at a camera site. \n\n\n# library(forcats)\nlibrary(ggplot2)\n# anns_wide_animals_joined_mut |&gt; \n#   ggplot(aes(x=date_time, y=species)) + geom_point()\n# anns_wide_animals_joined_mut |&gt; \n#   dplyr::filter(species == \"Fisher\") |&gt; View()\n\ncamera_days &lt;- exif |&gt; dplyr::group_by(maker_notes_user_label) |&gt; \n  dplyr::summarise(camera_first_time=min(exif_date_time_original)) |&gt; \n  dplyr::mutate(date_time = lubridate::ymd_hms(camera_first_time,\n                                               tz = \"Canada/Eastern\"),\n                year = lubridate::year(date_time),\n                month = lubridate::month(date_time, label = TRUE),\n                day = lubridate::day(date_time),\n                ymd = paste(year, month, day, sep = \"_\")) |&gt; \n  dplyr::select(camera_id = maker_notes_user_label, \n                camera_first_day = date_time)\n\ncamera_regions &lt;- cams_sf |&gt; sf::st_join(zones) |&gt; \n  sf::st_drop_geometry() |&gt; dplyr::select(camera_id, ECOZONE_NA)\n\nbp &lt;- anns_wide_animals_joined_mut |&gt; \n  \n  dplyr::filter(species != \"Fisher\") |&gt; \n  dplyr::left_join(camera_days, by = \"camera_id\") |&gt; \n\n  \n  dplyr::select(species, camera_id, \n                date_time, camera_first_day, ymd) |&gt; \n  \n  dplyr::group_by(species, camera_id) |&gt; \n  dplyr::summarise(first_day = min(date_time),\n                   camera_first_day = min(camera_first_day),\n                   camera_events = length(unique(ymd))) |&gt; \n  dplyr::ungroup() |&gt; \n  \n  dplyr::left_join(camera_regions, by = \"camera_id\") |&gt; \n  \n  dplyr::group_by(species) |&gt; \n  dplyr::mutate(total_events = sum(camera_events),\n                n_cameras = dplyr::n()) |&gt; \n  dplyr::ungroup() |&gt;\n  \n  # dplyr::filter(n_cameras &gt; 4) |&gt;\n  dplyr::filter(n_cameras &gt; 3) |&gt;\n  \n  dplyr::mutate(diff_time = difftime(first_day, camera_first_day, units=\"days\"),\n                ECOZONE_NA = as.factor(ECOZONE_NA)) |&gt; \n  \n  ggplot(aes(x = diff_time, \n             y = forcats::fct_reorder(species, total_events),\n             color = ECOZONE_NA)) +\n  geom_boxplot() +\n  \n  scale_x_continuous()+\n  labs(x = \"Detection time (in days)\",\n       y = \"Species\",\n       color = \"Ecozone\") + \n  theme_bw()\n\nggsave(plot = bp, filename = \"figures/bp.png\", width = 10, height = 15)\n\npts &lt;- anns_wide_animals_joined_mut |&gt; \n  \n  dplyr::filter(species != \"Fisher\") |&gt; \n  dplyr::left_join(camera_days, by = \"camera_id\") |&gt; \n\n  \n  dplyr::select(species, camera_id, \n                date_time, camera_first_day, ymd) |&gt; \n  \n  dplyr::group_by(species, camera_id) |&gt; \n  dplyr::summarise(first_day = min(date_time),\n                   camera_first_day = min(camera_first_day),\n                   camera_events = length(unique(ymd))) |&gt; \n  dplyr::ungroup() |&gt; \n  \n  dplyr::left_join(camera_regions, by = \"camera_id\") |&gt; \n  \n  dplyr::group_by(species) |&gt; \n  dplyr::mutate(total_events = sum(camera_events),\n                n_cameras = dplyr::n()) |&gt; \n  dplyr::ungroup() |&gt;\n  \n  # dplyr::filter(n_cameras &gt; 4) |&gt;\n  dplyr::filter(n_cameras &gt; 3) |&gt;\n  \n  dplyr::mutate(diff_time = difftime(first_day, camera_first_day, units=\"days\"),\n                ECOZONE_NA = as.factor(ECOZONE_NA)) |&gt; \n  \n  ggplot(aes(x = diff_time, \n             y = forcats::fct_reorder(species, total_events),\n             # size = camera_events, \n             color = ECOZONE_NA)) +\n\n  geom_point(aes(x = diff_time, \n                  y = forcats::fct_reorder(species, total_events),\n                  size = camera_events, \n                  color = ECOZONE_NA,\n                  group = ECOZONE_NA)) +\n  \n  scale_x_continuous()+\n  labs(x = \"Detection time (in days)\",\n       y = \"Species\",\n       color = \"Ecozone\",\n       size = \"Day counts\") + \n  theme_bw()\nggsave(plot = pts, filename = \"figures/pts.png\", width = 10, height = 15)\npts\n\n\n# for_vic &lt;- anns_wide_animals_joined_mut |&gt; \n#   dplyr::select(id,species,date_time, camera_id) |&gt; \n#   dplyr::left_join(cams_sf |&gt; dplyr::select(camera_id, plot_id) |&gt; sf::st_drop_geometry(),\n#                    by = \"camera_id\")\n# tot_camera &lt;- for_vic$camera_id |&gt; unique() |&gt; length()\n# hit_rates &lt;- for_vic |&gt; dplyr::select(species, camera_id) |&gt; \n#   dplyr::group_by(species) |&gt; dplyr::summarise(n_cameras=length(unique(camera_id))) |&gt; \n#   dplyr::ungroup() |&gt; dplyr::mutate(rate = n_cameras/tot_camera*100) |&gt; \n#   dplyr::arrange(dplyr::desc(rate))\n# # det_history_two_weeks &lt;- \n# \n# readr::write_csv(for_vic, \"../data/derived_data/observations_by_camera_2023Aug15.csv\")\n# readr::write_csv(hit_rates, \"../data/derived_data/hit_rates_by_species_2023Aug15.csv\")\n\n\n\nOrdination and clustering\nWe analysed the relative abundance data using well known numerical ecology community analysis methods (Legendre and Legendre 2013). As a first step, we combined clustering and ordination to visualize associations between species, camera sites, and land cover in the vicinity of the detection site. We used Ward clustering on the relative abundances matrix. We extracted land cover information in a 1-km radius around each camera site, using the Far North Land Cover data set from Ontario’s Ministry of Natural Resources and Forestry. We computed the frequencies of land cover classes and used these as constraining variable in a redundancy analysis, a method well suited for multidimensional data.",
    "crumbs": [
      "GnC Report"
    ]
  },
  {
    "objectID": "reports/GnC_report/paper/paper.html#data-exploration-and-preliminary-results",
    "href": "reports/GnC_report/paper/paper.html#data-exploration-and-preliminary-results",
    "title": "Using Camera Traps and Computer Vision to Quantify Wildlife Diversity and Co-Occurrence across Ontario’s Far North",
    "section": "Data exploration and preliminary results",
    "text": "Data exploration and preliminary results\n\nWildlife community compposition\nWe obtained repeat detections of at least 31 mammal and bird species across 193 sites and 5-7 months of detections The most common species detected (detected more than 10 days in the season) are listed in Table 2 below (the full table is available in the appendix at the end of this report). Note the large amount detections in the “bird” and “duck” species categories which could be further distinguished into specific species.\n\n\n\n\n\n\n\n\nTable 2: Relative abundances for species with more than 10 detection days\n\n\nComputed in number of days detected in the season\n\n\nSpecies\nScientific name\nDays\n\n\n\n\nSandhill crane\nAntigone canadensis\n240\n\n\nCaribou\nRangifer tarandus\n211\n\n\nBird sp\nNA\n125\n\n\nMoose\nAlces americanus\n105\n\n\nCanada Jay\nPerisoreus canadensis\n89\n\n\nSharp-tailed grouse\nTympanuchus phasianellus\n72\n\n\nCanada Goose\nBranta canadensis\n59\n\n\nAmerican marten\nMartes americana\n57\n\n\nRed squirrel\nTamiasciurus hudsonicus\n49\n\n\nSnowshoe hare\nLepus americanus\n47\n\n\nDuck sp\nNA\n42\n\n\nBlack bear\nUrsus americanus\n33\n\n\nMustelid sp\nNA\n29\n\n\nMallard\nAnas platyrhynchos\n28\n\n\nRed fox\nVulpes vulpes\n14\n\n\nSpruce grouse\nFalcipennis canadensis\n11\n\n\nWolf\nNA\n10\n\n\n\n\n\n\n\nAs for the spatial distribution of those relative abundances, Figure 3 shows relative species abundances across the landscape, for species detected more than 50 days in the season (this number is roughly equal to the standard deviation of the distribution of detection days, which has a mean of 32 days). Note that for this figure, the detections have grouped by plot number for better visibility, with a plot containing up to 4 cameras (see deployment methods). There are no clear spatial patterns, but some generalities can be noted: Moose detections are predominant in the northwestern and southeastern zones of the sampling array, while caribou and sandhill crane detections share the most of the detections for the central part of the array.\n\n\n\n\n\n\n\n\nFigure 3: Pie chart map of relative species abundances across our study area Detections have grouped by plot for better visibility, with a plot containing up to 4 cameras. Only species detected more than 50 days in the season are shown for clarity.\n\n\n\n\n\n\n\nOrdination - presence-absence\nTo start exploring general species-habitat associations we completed an redundancy analysis (RDA, using the vegan R package, Oksanen et al. (2023)). RDAs allow to study the relationships between two matrices of data, and is often used in community studies, using a constraining matrix of environmental variables for each site and a corresponding matrix of species observations (either presence absence or relative abundances). Ordinations also combine very well with clustering analyses to produce insights about associations between sites.\nWe used the Ontario Far North Land Cover dataset to produce proportions of land covers at each site (Hogg (2014), a full legend of the land cover abbreviations are to be found in the appendix of this report). Figure 4 shows the triplot for the RDA on presence-absence data, which yielded a low \\(R^2\\) of 0.05, but which is still relevant for exploring species associations. Land cover might not be able to explain community structure, but it can still show which species are found together and around what kind of land cover class. Here, we see that caribou and sandhill cranes are associated with open and treed wetlands such as open and treed bogs and fens (OBOG, OFEN, TrBOG, and TrFEN) while black bear and moose are found together in more forested areas of mixed trees (MixTRE), sparse Trees (SpTRE) and coniferous trees (ConTRE). Canada geese are, as is to be expected, associated with water (WAT) while smaller species like squirrels and martens score away from the larger mammals in a mix of habitats, including swamps (SWA).\n\n\n\n\n\n\n\n\nFigure 4: Triplot of wildlife presence-absence data from Northern Ontario camera trapping images collected in March through September of 2022 reveals that different species associate with different land cover classes. For example, caribou cranes are more likely to be detected near open and treed wetlands (OBOG, OFEN, TrBOG, and TrFEN) while black bear and moose near forested areas (MixTRE, SpTRE, ConTRE).\n\n\n\n\n\n\n\nOrdination - relative abundances\nTo continue exploring abundance-habitat relationships as a proxy for species relative habitat use, we also conducted an RDA using relative abundances instead of presence absence. Figure 5 shows the triplot for the RDA on relative abundance data. This analysis yielded a slightly larger \\(R^2\\) of 0.06, than the analysis in Figure 4 presenting presence-absence data, but reduces the axis scores of the rarer species. We observe similar patterns for caribou, crane, moose, black bear and marten. \n\n\n\n\n\n\n\n\nFigure 5: Triplot of wildlife relative abundance data from Northern Ontario camera trapping images collected in March through September of 2022, showing similar patterns than the triplot using presence-absence data.\n\n\n\n\n\n\n\nClustering\nTo start inferring patters from the above ordination analyses we used a clustering algorithm on the relative abundance data presented in Figure 5. Figure 6 shows the triplot for the RDA on relative abundance data, with Ward clusters as ellipses (Legendre and Legendre (2012)). We divided our ordination plot into 2 clusters along the first RDA axis. One cluster for sites primarily found with caribou and cranes, and a second cluster for the rest of the cameras and species.This clustering generally reflects the spatial patterns in species abundance that we observed in Figure 3.\n\n\n\n\n\n\n\n\n\nFigure 6: Triplot of wildlife relative abundance data from Northern Ontario camera trapping images collected in March through September of 2022, showing similar patterns than the triplot using presence-absence data. Two clusters determined using Ward clustering are shown as ellipses.\n\n\n\n\n\nThese clusters can be visualized on a map, which Figure 7 shows. There is no apparent discernible spatial pattern.\n\n\n\n\n\n\n\n\nFigure 7: Map of clustered camera sites. The two clusters were determined using the Ward clustering method.\n\n\n\n\n\n\n\nSpatial clustering\nSpatial clustering allows to add distance between sites as a soft constraint in the clustering algorithm. The extent to which these distances influence the result in balance with the community data can be adjusted, by setting the strength of the mixing parameter alpha. The ClustGeo package (Chavent et al. (2018)) allows to test different values of that parameter to determine optimal mixing of the clustering forces, with the function choicealpha.\nWe found that the optimal value for alpha be 0.1, and performed mixed clustering using the hclustgeo function. Figure 8 shows the resulting map of geographically constrained clustered camera sites. This reveals a North/South pattern in the spatial clustering of sites.\n\n\n\n\n\n\n\n\nFigure 8: Map of geographically constrained clustered camera sites, using geographically constrained clustering with a mixing parameter of 0.1.\n\n\n\n\n\nWe can plot those geographically constrained clusters on top of the abundance-based RDA of Figure 5. Figure 9 shows that those geographic clusters do not map well on top of the ordination, with one cluster seeming to represent a subset of a larger trend experienced in the larger part of the data.\n\n\n\n\n\n\n\n\nFigure 9: Triplot of wildlife relative abundance data from Northern Ontario camera trapping images collected in March through September of 2022, showing similar patterns than the triplot using presence-absence data. Two clusters determined using geographically constrained clustering with a mixing parameter of 0.1.\n\n\n\n\n\n\n\nSpecies heatmaps and bubbleplots\nIn order to visualize relative abundances through time and space, we can use bubbleplots. The following plots are for caribou, sandhill crane, moose, and wolf. They represent days detected at each camera site per month. \n \n \n \n \nAs more of the camera images are labeled and our dataset grows, we can transform the above bubble plots into heat maps of species presence/absence or relative abundance. Below is an example using the existing caribou data to date. It is important to recognize that not all cameras had been deployed until June, leaving April and Ma. y maps with large portions of the northern study area where cameras were not yet present. \n\n\nTake Home messages\nWe can derive a few take home messages from our initial exploration of the data:\n\nUsing an object detection model allowed us to filter out large quantities of empty images, but performed somewhat poorly in certain conditions (namely, windy sites with drastic changes in vegetation).\nAccording to our analysis, species tend to associate with land use changes classes that generally corresponds to their expected habitat, such as wetlands for caribou and cranes, or forested areas for moose.\nClustering reveals two large groups among sites, but spatial patterns are difficult to tease apart at this stage.",
    "crumbs": [
      "GnC Report"
    ]
  },
  {
    "objectID": "reports/GnC_report/paper/paper.html#next-steps",
    "href": "reports/GnC_report/paper/paper.html#next-steps",
    "title": "Using Camera Traps and Computer Vision to Quantify Wildlife Diversity and Co-Occurrence across Ontario’s Far North",
    "section": "Next steps",
    "text": "Next steps\nOur existing image identification workflow and tagging process has allowed us to produce an open and reproducible data set of spatially explicit wildlife species counts in Ontario’s Far North. Using this data set we have started to explore spatial patterns of species richness, occurrence, and abundance. All of this work is conducted through opensource platforms, and data exploration and analysis code hosted as a GitHub repository for ease of access and reproducibilty. The information necessay to reproduce this report is in the colophon in the appendix.\nOver the coming months we will continue to process the retrieved data and anticipate adding to this data set over the summer, as data from 18 cameras has yet to be retrieved. The next steps for this project are to complete the tagging process and identify all the detections tagged as unknowns, bird, duck and mustelid species. We will also further explore the spatial community composition patterns at each camera site. Given a high enough data density for certain species future possibilities for this project include modelling species occupancy (MacKenzie et al. (2002)) and spatially explicit density estimation using spatially explicit capture-recapture models (Chandler and Royle (2013)). \nAfter data processing and exploration is complete the next major step in this project will be to use the bounding boxes information we have generated to train an image classifier. This classifier will be based on Megaclassifier (of type EfficientNet architecture, Tan and Le (2019)), an classifier already trained to pair with Megadetector’s output. We will retrain this model to fit our taxa, possibly leveraging mentoring opportunities offered by the CV4E workshop (“Computer Vision for Ecology”), if availability allows.",
    "crumbs": [
      "GnC Report"
    ]
  },
  {
    "objectID": "reports/GnC_report/paper/paper.html#acknowledgements",
    "href": "reports/GnC_report/paper/paper.html#acknowledgements",
    "title": "Using Camera Traps and Computer Vision to Quantify Wildlife Diversity and Co-Occurrence across Ontario’s Far North",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe thank the student volunteers at Wilfrid Laurier University for their help with tagging: Teea Curlew, Bridget Matthews, Meghna Pal, Rafay Siraj, and Dietrich Westberg. We also thank members of the WILDlab for feedback throughout this process: Claudia Haas, Eric Jolin, Charlotte Rentmeister, and Sheyda Zand. This work was supported by a G&C to Frances Stewart.",
    "crumbs": [
      "GnC Report"
    ]
  },
  {
    "objectID": "reports/GnC_report/paper/paper.html#references",
    "href": "reports/GnC_report/paper/paper.html#references",
    "title": "Using Camera Traps and Computer Vision to Quantify Wildlife Diversity and Co-Occurrence across Ontario’s Far North",
    "section": "References",
    "text": "References\n\n\n\nBeery, Sara, Dan Morris, and Siyu Yang. 2019. “Efficient Pipeline for Camera Trap Image Review.” arXiv Preprint arXiv:1907.06772.\n\n\nChandler, Richard B., and J. Andrew Royle. 2013. “Spatially Explicit Models for Inference about Density in Unmarked or Partially Marked Populations.” The Annals of Applied Statistics 7 (2). https://doi.org/10.1214/12-aoas610.\n\n\nChavent, Marie, Vanessa Kuentz-Simonet, Amaury Labenne, and Jérôme Saracco. 2018. “ClustGeo: An r Package for Hierarchical Clustering with Spatial Constraints.” Computational Statistics 33 (4): 1799–1822. https://doi.org/10.1007/s00180-018-0791-1.\n\n\nDam-Bates, Paul van, Oliver Gansell, and Blair Robertson. 2018. “Using Balanced Acceptance Sampling as a Master Sample for Environmental Surveys.” Edited by Robert Freckleton. Methods in Ecology and Evolution 9 (7): 1718–26. https://doi.org/10.1111/2041-210x.13003.\n\n\nHogg, Adam R. 2014. “Far North Land Cover (Version 1.2) Accuracy Assessment Report.” Peterborough. Ontario.\n\n\nLegendre, Pierre, and Louis Legendre. 2012. “Cluster Analysis.” In Developments in Environmental Modelling, 337–424. Elsevier. https://doi.org/10.1016/b978-0-444-53868-0.50008-3.\n\n\nMacKenzie, Darryl I., James D. Nichols, Gideon B. Lachman, Sam Droege, J. Andrew Royle, and Catherine A. Langtimm. 2002. “ESTIMATING SITE OCCUPANCY RATES WHEN DETECTION PROBABILITIES ARE LESS THAN ONE.” Ecology 83 (8): 2248–55. https://doi.org/10.1890/0012-9658(2002)083[2248:esorwd]2.0.co;2.\n\n\nOksanen, Jari, Gavin L. Simpson, F. Guillaume Blanchet, Roeland Kindt, Pierre Legendre, Peter R. Minchin, R. B. O’Hara, et al. 2023. Vegan: Community Ecology Package. https://github.com/vegandevs/vegan.\n\n\nSekachev, Boris, Nikita Manovich, Maxim Zhiltsov, Andrey Zhavoronkov, Dmitry Kalinin, Ben Hoff, TOsmanov, et al. 2020. “Opencv/Cvat: V1.1.0.” Zenodo. https://doi.org/10.5281/zenodo.4009388.\n\n\nTan, Mingxing, and Quoc V. Le. 2019. “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.” https://doi.org/10.48550/ARXIV.1905.11946.\n\n\nTkachenko, Maxim, Mikhail Malyuk, Andrey Holmanyuk, and Nikolai Liubimov. 2020-2022. “Label Studio: Data Labeling Software.” https://github.com/heartexlabs/label-studio.",
    "crumbs": [
      "GnC Report"
    ]
  },
  {
    "objectID": "reports/GnC_report/paper/paper.html#appendix",
    "href": "reports/GnC_report/paper/paper.html#appendix",
    "title": "Using Camera Traps and Computer Vision to Quantify Wildlife Diversity and Co-Occurrence across Ontario’s Far North",
    "section": "Appendix",
    "text": "Appendix\n\n\n\n\n\n\n\n\nTable 3: Relative species abundances\n\n\nComputed in number of days detected in the season\n\n\nSpecies\nDays\n\n\n\n\nSandhill crane\n240\n\n\nCaribou\n211\n\n\nBird sp\n125\n\n\nMoose\n105\n\n\nCanada Jay\n89\n\n\nSharp-tailed grouse\n72\n\n\nCanada Goose\n59\n\n\nAmerican marten\n57\n\n\nRed squirrel\n49\n\n\nSnowshoe hare\n47\n\n\nDuck sp\n42\n\n\nBlack bear\n33\n\n\nMustelid sp\n29\n\n\nMallard\n28\n\n\nRed fox\n14\n\n\nSpruce grouse\n11\n\n\nWolf\n10\n\n\nShorebird sp.\n8\n\n\nWolverine\n6\n\n\nRobin\n6\n\n\nRuffed grouse\n5\n\n\nWood duck\n5\n\n\nOwl\n4\n\n\nRaven\n4\n\n\nBonaparte's gull\n4\n\n\nCommon goldeneye\n4\n\n\nRiver otter\n3\n\n\nDark eyed Junco\n3\n\n\nCedar waxwing\n3\n\n\nGreen winged teal\n3\n\n\nHooded merganser\n3\n\n\nBeaver\n3\n\n\nThrush sp.\n3\n\n\nGreat grey Owl\n2\n\n\nCanada Lynx\n2\n\n\nRing-necked duck\n2\n\n\nCommon Merganser\n2\n\n\nMuskrat\n2\n\n\nEastern chipmunk\n2\n\n\nFisher\n2\n\n\nNorthern Harrier\n1\n\n\nFox sp\n1\n\n\nLaughing gull\n1\n\n\nPie-billed grebe\n1\n\n\nShorebird sp\n1\n\n\nMerganser sp.\n1\n\n\nBald eagle\n1\n\n\nRaptor sp.\n1\n\n\nCommon grackle\n1\n\n\nWhite-throated sparrow\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4: Land cover classes abbreviations corresponding to the Ontatio Far North Land cover dataset.\n\n\nAdapted from the ontario Far North land cover dataset\n\n\nAbbreviations\nLand cover class\n\n\n\n\nAGRI\nAgriculture\n\n\nBED\nBedrock\n\n\nCLOUD\nCloud/Shadow\n\n\nConSWA\nConiferous Swamp\n\n\nConTRE\nConiferous Treed\n\n\nDecSWA\nDeciduous Swamp\n\n\nDecTRE\nDeciduous Treed\n\n\nFrMAR\nFreshwater Marsh\n\n\nHEATH\nHeath\n\n\nInMAR\nIntertidal Marsh\n\n\nMIN\nSand/Gravel/Mine Tailings\n\n\nMUD\nIntertidal Mudflat\n\n\nMixTRE\nMixed Treed\n\n\nNSWood\nDisturbance - Non and Sparse Woody\n\n\nOBOG\nOpen Bog\n\n\nOFEN\nOpen Fen\n\n\nOTH\nOther\n\n\nSpTRE\nSparse Treed\n\n\nSuMAR\nSupertidal Marsh\n\n\nThSWA\nThicket Swamp\n\n\nTrBOG\nTreed Bog\n\n\nTrFEN\nTreed Fen\n\n\nTrOrSHr\nDisturbance - Treed and/or Shrub\n\n\nURB\nCommunity/Infrastructure\n\n\nWAT\nClear Open Water\n\n\nXWAT\nTurbid Water\n\n\n\n\n\n\n\n\n\n\nColophon\nThis report was generated on 2024-01-28 13:12:37.575224 using the following computational environment and dependencies:\n\n# which R packages and versions?\nif (\"devtools\" %in% installed.packages()) devtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.2 (2023-10-31)\n os       Pop!_OS 22.04 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  en_CA.UTF-8\n ctype    en_CA.UTF-8\n tz       America/Toronto\n date     2024-01-28\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package       * version date (UTC) lib source\n P bit             4.0.5   2022-11-15 [?] CRAN (R 4.3.1)\n P bit64           4.0.5   2020-08-30 [?] CRAN (R 4.3.1)\n P bitops          1.0-7   2021-04-24 [?] CRAN (R 4.3.1)\n P cachem          1.0.8   2023-05-01 [?] CRAN (R 4.3.1)\n P callr           3.7.3   2022-11-02 [?] CRAN (R 4.3.1)\n P cellranger      1.1.0   2016-07-27 [?] CRAN (R 4.3.1)\n P class           7.3-22  2023-05-03 [?] CRAN (R 4.3.1)\n P classInt        0.4-10  2023-09-05 [?] CRAN (R 4.3.1)\n P cli             3.6.1   2023-03-23 [?] CRAN (R 4.3.1)\n P cluster         2.1.6   2023-12-01 [?] CRAN (R 4.3.2)\n P ClustGeo        2.1     2021-09-30 [?] CRAN (R 4.3.1)\n P codetools       0.2-19  2023-02-01 [?] CRAN (R 4.2.2)\n P colorspace      2.1-0   2023-01-23 [?] CRAN (R 4.3.1)\n P crayon          1.5.2   2022-09-29 [?] CRAN (R 4.3.1)\n   curl            5.1.0   2023-10-02 [1] CRAN (R 4.3.2)\n P DBI             1.1.3   2022-06-18 [?] CRAN (R 4.3.1)\n P devtools        2.4.5   2022-10-11 [?] CRAN (R 4.3.1)\n P digest          0.6.33  2023-07-07 [?] CRAN (R 4.3.1)\n P dplyr           1.1.4   2023-11-17 [?] CRAN (R 4.3.2)\n P e1071           1.7-13  2023-02-01 [?] CRAN (R 4.3.1)\n P ellipsis        0.3.2   2021-04-29 [?] CRAN (R 4.3.1)\n P evaluate        0.23    2023-11-01 [?] CRAN (R 4.3.2)\n P exactextractr   0.10.0  2023-09-20 [?] CRAN (R 4.3.2)\n   fansi           1.0.5   2023-10-08 [1] CRAN (R 4.3.2)\n P farver          2.1.1   2022-07-06 [?] CRAN (R 4.3.1)\n P fastmap         1.1.1   2023-02-24 [?] CRAN (R 4.3.1)\n P fs              1.6.3   2023-07-20 [?] CRAN (R 4.3.1)\n P generics        0.1.3   2022-07-05 [?] CRAN (R 4.3.1)\n P ggforce         0.4.1   2022-10-04 [?] CRAN (R 4.3.1)\n P ggfun           0.1.3   2023-09-15 [?] CRAN (R 4.3.2)\n P ggmap           4.0.0   2023-11-19 [?] CRAN (R 4.3.2)\n   ggplot2         3.4.4   2023-10-12 [1] CRAN (R 4.3.2)\n P ggrepel         0.9.4   2023-10-13 [?] CRAN (R 4.3.1)\n P glue            1.6.2   2022-02-24 [?] CRAN (R 4.3.1)\n P gt              0.10.0  2023-10-07 [?] CRAN (R 4.3.2)\n P gtable          0.3.4   2023-08-21 [?] CRAN (R 4.3.1)\n P hms             1.1.3   2023-03-21 [?] CRAN (R 4.3.1)\n P htmltools       0.5.7   2023-11-03 [?] CRAN (R 4.3.2)\n P htmlwidgets     1.6.3   2023-11-22 [?] CRAN (R 4.3.2)\n P httpuv          1.6.12  2023-10-23 [?] CRAN (R 4.3.2)\n P httr            1.4.7   2023-08-15 [?] CRAN (R 4.3.1)\n P janitor         2.2.0   2023-02-02 [?] CRAN (R 4.3.1)\n P jpeg            0.1-10  2022-11-29 [?] CRAN (R 4.3.1)\n P jsonlite        1.8.7   2023-06-29 [?] CRAN (R 4.3.1)\n P KernSmooth      2.23-22 2023-07-10 [?] CRAN (R 4.3.1)\n P knitr           1.45    2023-10-30 [?] CRAN (R 4.3.2)\n P labeling        0.4.3   2023-08-29 [?] CRAN (R 4.3.1)\n P later           1.3.1   2023-05-02 [?] CRAN (R 4.3.1)\n P lattice         0.22-5  2023-10-24 [?] CRAN (R 4.3.1)\n   lifecycle       1.0.4   2023-11-07 [1] CRAN (R 4.3.2)\n   lubridate       1.9.3   2023-09-27 [1] CRAN (R 4.3.2)\n P magrittr      * 2.0.3   2022-03-30 [?] CRAN (R 4.3.1)\n P mapproj         1.2.11  2023-01-12 [?] CRAN (R 4.3.1)\n   maps            3.4.1.1 2023-11-03 [1] CRAN (R 4.3.2)\n P MASS            7.3-60  2023-05-04 [?] CRAN (R 4.3.1)\n P Matrix          1.6-3   2023-11-14 [?] CRAN (R 4.3.2)\n P memoise         2.0.1   2021-11-26 [?] CRAN (R 4.3.1)\n P mgcv            1.9-1   2023-12-21 [?] CRAN (R 4.3.2)\n P mime            0.12    2021-09-28 [?] CRAN (R 4.3.1)\n P miniUI          0.1.1.1 2018-05-18 [?] CRAN (R 4.3.1)\n P munsell         0.5.0   2018-06-12 [?] CRAN (R 4.3.1)\n P nlme            3.1-163 2023-08-09 [?] CRAN (R 4.3.1)\n P permute         0.9-7   2022-01-27 [?] CRAN (R 4.3.1)\n P pillar          1.9.0   2023-03-22 [?] CRAN (R 4.3.1)\n P pkgbuild        1.4.2   2023-06-26 [?] CRAN (R 4.3.1)\n P pkgconfig       2.0.3   2019-09-22 [?] CRAN (R 4.3.1)\n P pkgload         1.3.3   2023-09-22 [?] CRAN (R 4.3.1)\n   plyr            1.8.9   2023-10-02 [1] CRAN (R 4.3.2)\n P png             0.1-8   2022-11-29 [?] CRAN (R 4.3.1)\n   polyclip        1.10-6  2023-09-27 [1] CRAN (R 4.3.2)\n P prettyunits     1.2.0   2023-09-24 [?] CRAN (R 4.3.1)\n P processx        3.8.2   2023-06-30 [?] CRAN (R 4.3.1)\n P profvis         0.3.8   2023-05-02 [?] CRAN (R 4.3.1)\n P promises        1.2.1   2023-08-10 [?] CRAN (R 4.3.1)\n P proxy           0.4-27  2022-06-09 [?] CRAN (R 4.3.1)\n P ps              1.7.5   2023-04-18 [?] CRAN (R 4.3.1)\n P purrr           1.0.2   2023-08-10 [?] CRAN (R 4.3.1)\n P R6              2.5.1   2021-08-19 [?] CRAN (R 4.3.1)\n P ragg            1.2.6   2023-10-10 [?] CRAN (R 4.3.1)\n P raster          3.6-26  2023-10-14 [?] RSPM (R 4.3.1)\n P Rcpp            1.0.11  2023-07-06 [?] CRAN (R 4.3.1)\n P readr           2.1.4   2023-02-10 [?] CRAN (R 4.3.1)\n P readxl          1.4.3   2023-07-06 [?] CRAN (R 4.3.1)\n P remotes         2.4.2.1 2023-07-18 [?] CRAN (R 4.3.1)\n P renv            1.0.3   2023-09-19 [?] CRAN (R 4.3.2)\n   rlang           1.1.2   2023-11-04 [1] CRAN (R 4.3.2)\n P rmarkdown       2.25    2023-09-18 [?] CRAN (R 4.3.1)\n P rstudioapi      0.15.0  2023-07-07 [?] CRAN (R 4.3.1)\n P s2              1.1.4   2023-05-17 [?] CRAN (R 4.3.1)\n P sass            0.4.7   2023-07-15 [?] CRAN (R 4.3.1)\n P scales          1.2.1   2022-08-20 [?] CRAN (R 4.3.1)\n P scatterpie      0.2.1   2023-06-07 [?] CRAN (R 4.3.1)\n P sessioninfo     1.2.2   2021-12-06 [?] CRAN (R 4.3.1)\n P sf              1.0-14  2023-07-11 [?] CRAN (R 4.3.1)\n P shiny           1.8.0   2023-11-17 [?] CRAN (R 4.3.2)\n P snakecase       0.11.1  2023-08-27 [?] CRAN (R 4.3.1)\n   sp              2.1-1   2023-10-16 [1] CRAN (R 4.3.2)\n P stringi         1.8.1   2023-11-13 [?] CRAN (R 4.3.2)\n P stringr         1.5.1   2023-11-14 [?] CRAN (R 4.3.2)\n P systemfonts     1.0.5   2023-10-09 [?] CRAN (R 4.3.1)\n P terra           1.7-55  2023-10-13 [?] RSPM (R 4.3.1)\n P textshaping     0.3.7   2023-10-09 [?] CRAN (R 4.3.1)\n P tibble          3.2.1   2023-03-20 [?] CRAN (R 4.3.1)\n P tidyr           1.3.0   2023-01-24 [?] CRAN (R 4.3.1)\n P tidyselect      1.2.0   2022-10-10 [?] CRAN (R 4.3.1)\n P timechange      0.2.0   2023-01-11 [?] CRAN (R 4.3.1)\n P tweenr          2.0.2   2022-09-06 [?] CRAN (R 4.3.1)\n P tzdb            0.4.0   2023-05-12 [?] CRAN (R 4.3.1)\n P units           0.8-4   2023-09-13 [?] CRAN (R 4.3.1)\n P urlchecker      1.0.1   2021-11-30 [?] CRAN (R 4.3.1)\n P usethis         2.2.2   2023-07-06 [?] CRAN (R 4.3.1)\n   utf8            1.2.4   2023-10-22 [1] CRAN (R 4.3.2)\n   vctrs           0.6.4   2023-10-12 [1] CRAN (R 4.3.2)\n P vegan           2.6-4   2022-10-11 [?] CRAN (R 4.3.1)\n P viridisLite     0.4.2   2023-05-02 [?] CRAN (R 4.3.1)\n P vroom           1.6.4   2023-10-02 [?] RSPM (R 4.3.1)\n   withr           2.5.2   2023-10-30 [1] CRAN (R 4.3.2)\n P wk              0.9.0   2023-10-22 [?] RSPM (R 4.3.1)\n P xfun            0.41    2023-11-01 [?] CRAN (R 4.3.2)\n P xml2            1.3.5   2023-07-06 [?] CRAN (R 4.3.1)\n P xtable          1.8-4   2019-04-21 [?] CRAN (R 4.3.1)\n P yaml            2.3.7   2023-01-23 [?] CRAN (R 4.3.1)\n\n [1] /home/vlucet/.cache/R/renv/library/rofcamtrap-cbeaf265/R-4.3/x86_64-pc-linux-gnu\n [2] /home/vlucet/.cache/R/renv/sandbox/R-4.3/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────\n\n\nThe current Git commit details are:\n\n# what commit is this file at? \nif (\"git2r\" %in% installed.packages() & \n    git2r::in_repository(path = \".\")) {\n  git2r::repository(here::here())  \n}\n\nLocal:    main /home/vlucet/Documents/WILDLab/rofcamtrap\nRemote:   main @ origin (git@github.com:StewartWILDlab/rofcamtrap.git)\nHead:     [52b2a47] 2024-01-28: update license",
    "crumbs": [
      "GnC Report"
    ]
  }
]