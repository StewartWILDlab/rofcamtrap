---
title: "Ring of Fire Camera Detections Report"
author:
  - Valentin Lucet:
      correspondence: "yes"
      email: valentin.lucet@gmail.com
      orcid: "0000-0003-0268-818X"
      institute: wlu
  - Frances Stewart:
      institute: wlu
      orcid: "0000-0001-9344-8346"
institute:
  - wlu:
      name: Wilfrid Laurier University
      # address: 23 Science Street, Eureka, Mississippi, USA
title-block-published: "Last updated"  
date: now
date-format: long
format: 
  docx:
    reference-doc: "../templates/template.docx" # Insert path for the DOCX file
execute:
  echo: true
  warning: false
  message: false
  comment: "#>"
  fig-path: "../figures/"
  fig-dpi: 600
filters:
  - ../templates/scholarly-metadata.lua
  - ../templates/author-info-blocks.lua
  - ../templates/pagebreak.lua
bibliography: references.bib
csl: "../templates/landscape-ecology.csl" # Insert path for the bib-style
abstract: |
  Abstract TBW
# keywords: |
#   keyword 1; keyword 2; keyword 3
# highlights: |
#   These are the highlights. 
---

```{r, setup, include=FALSE}
knitr::opts_knit$set(root.dir = '~/Documents/WILDLab/rofcamtrap/')
options(scipen = 1, digits = 2)
```

<!-- This is the format for text comments that will be ignored during renderings. Do not put R code in these comments because it will not be ignored.

With the following code you can access and display values from the yml header above.
Keywords: `r rmarkdown::metadata$keywords`
Highlights: `r rmarkdown::metadata$highlights`

Here is a citation [@Marwick2017] -->

<!-- The actual document text starts here: -->

## Summary

TBW

## Introduction

- What this report is about
- Recycle the other introductions
- Importance of monitoring
- Data collection challenge
- New methods to the rescue
- Data challenges remain
- AI plan

## Methods

### Sampling protocol

- Recycle the info from proposal

### Image detection

```{r}
#| label: load-data
#| echo: false

if (!file.exists("analysis/data/derived_data/annotations.rds")){
  source("scripts/1_preprocess_annotations.R")
} else {
  anns <- readRDS("analysis/data/derived_data/annotations.rds")
}

if (!file.exists("analysis/data/derived_data/detections.rds")){
  source("scripts/2_preprocess_detections.R")
} else {
  dets <- readRDS("analysis/data/derived_data/detections.rds")
}

# Compute total amount of images
nb_image <- length(unique(dets$image_id))
nb_image_fmt <- prettyNum(nb_image, big.mark = ",")

# Hardcoded check
stopifnot(nb_image == 2788375)
```

Images were retrieved from all cameras using a SD card reader and copied onto a 10 TB hard drive of brand LaCie. The images amounted to 1.8TB of memory on disk, for a total of __`r nb_image_fmt`__ images. Images were stored with the following directory structure:

- Plot id (e.g. P128)
- Site id (e.g. P128-1)
- Camera id (e.g. CFS-10)
- DCIM folder (created by the camera)
- Overflow folder (e.g. 100RECNX, each such folder contains up to 10,000 images)

The images were processed through the animal detection model MegaDetector, version 5.0, using a Lenovo X1 laptop with a NVIDIA GeForce GTX 1650 Max-Q graphics card, which took about 2 weeks of quasi-continuous processing to complete. The model processes each image and produces “detections” (bounding boxes) at a  variable confidence score.

```{r}
#| label: basics-stats-1
#| echo: false

# Select and distinct down to image-based infos
dets_images <- dets |>  dplyr::select(image_id, max_confidence, isempty) |> 
  dplyr::distinct()

# Breakdown of which images triggered a MD detection
dets_tb <- dets_images$isempty |> table()
dets_tb_sum <- sum(dets_tb)
dets_tb_percent <- dets_tb/dets_tb_sum*100

dets_tb_fmt <- prettyNum(dets_tb, big.mark = ",")
```

About __`r dets_tb_percent[1]` %__ of the images, or __`r dets_tb_fmt[1]`__ images, were found by the model to possibly have either a person, animal, or vehicle. The model did not produce detections for __`r dets_tb_fmt[2]`__ images, or __`r dets_tb_percent[2]` %__ of the total.

```{r}
#| label: basics-stats-2
#| echo: false
dets_0 <- dets |> dplyr::filter(confidence > 0)
dets_0.1 <- dets |> dplyr::filter(confidence > 0.1)

# Breakdown of detections by categories (animal, person, vehicles)
dets_tb_cat_unfil <- dets_0 |> dplyr::pull(category_id) |> table()
dets_tb_cat_unfil_sum <- sum(dets_tb_cat_unfil)
dets_tb_cat_unfil_percent <- dets_tb_cat_unfil/dets_tb_cat_unfil_sum*100

dets_tb_cat_unfil_fmt <- prettyNum(dets_tb_cat_unfil, big.mark = ",")
dets_tb_cat_unfil_sum_fmt <- prettyNum(dets_tb_cat_unfil_sum, big.mark = ",")

# Breakdown of detections above 0.1 by categories (animal, persons, vehicles)
dets_tb_cat <- dets_0.1 |> dplyr::pull(category_id) |> table()
dets_tb_cat_sum <- sum(dets_tb_cat)
dets_tb_cat_percent <- dets_tb_cat/sum(dets_tb_cat)*100

dets_tb_cat_fmt <- prettyNum(dets_tb_cat, big.mark = ",")
dets_tb_cat_sum_fmt <- prettyNum(dets_tb_cat_sum, big.mark = ",")
```

This resulted in a total of __`r dets_tb_cat_unfil_sum_fmt`__ detections. The totals for each category are: __`r dets_tb_cat_unfil_fmt[1]`__ animals (or __`r dets_tb_cat_unfil_percent[1]` %__); __`r dets_tb_cat_unfil_fmt[2]`__ persons (or __`r dets_tb_cat_unfil_percent[2]` %__); and __`r dets_tb_cat_unfil_fmt[3]`__ vehicles (or __`r dets_tb_cat_unfil_percent[3]` %__).

Using a cutt-off value for confidence threshold of 0.1, this left us with a total of __`r dets_tb_cat_sum_fmt`__ detections to sort through (see @fig-detections-histogram for an histogram of the detection scores, grouped by detection types). The totals for each category are: __`r dets_tb_cat_fmt[1]`__ animals (or __`r dets_tb_cat_percent[1]` %__); __`r dets_tb_cat_fmt[2]`__ persons (or __`r dets_tb_cat_percent[2]` %__); and __`r dets_tb_cat_fmt[3]`__ vehicles (or __`r dets_tb_cat_percent[3]` %__). 

```{r}
#| label: fig-detections-histogram
#| echo: false
#| fig-cap: "Histogram of detection confidence scores."

# Histogram of confidence scores above 0.1
dets_0.1 |> 
  dplyr::mutate(category_id = as.character(category_id)) |> 
  dplyr::mutate(Type = dplyr::case_match(category_id,
                                         "0" ~ "Empty",
                                         "1" ~ "Animal",
                                         "2" ~ "Person",
                                         "3" ~ "Vehicle")) |> 
  ggplot2::ggplot() +
  ggplot2::theme_bw() +
  ggplot2::geom_histogram(ggplot2::aes(x=confidence, group=Type, fill=Type),
                          bins = 100) +
  ggplot2::scale_fill_viridis_d() +
  ggplot2::xlab("Confidence Score") +
  ggplot2::ylab("Count")
```

- Add explanation about image bursts and upload of bursts including empties

### Tagging platform selection

We had in mind the following criteria when selecting the proper tool to tag our images and bounding boxes:

- Deployable online. This was to allow for for multiple taggers
- Flexible tagging interface. Few platforms allow for a full customization of the tagging interface, down to the smallest details.
- Allowing to tag multiple bounding boxes per image. Most platforms only allow for tagging the entire image, and not for portions of it (e.g. bounding boxes of detections produced by MegaDetector). This is necessary for when we will crop our images to train our neural network model. 
- Optionally, open source, to allow for maximum reproducibility of the process. 

We looked at platforms not traditionally used for tagging camera trap images, but used by the larger community of machine learning practitioners for which the task of tagging is a common step, known as labeling. We found two platforms that matched all those criteria: LabelStudio and CVAT. After testing both platforms, LabelStudio was chosen as it was simpler to implement. The Community edition of LabelStudio was deployed on an Ubuntu virtual machine hosted by Compute Canada, using an Apache server and a NGINX reverse proxy, which is a standard approach for custom hosting of websites.

### Tagging progress

```{r}
#| label: basics-stats-3
#| echo: false

# Number of uploaded images
uploaded <- dets |> dplyr::filter(confidence > 0.1) |> dplyr::pull(image_id) |> 
  unique() |> length()

uploaded_fmt <- prettyNum(uploaded, big.mark = ",")

# Check total number of images
stopifnot(dets_tb_sum == length(unique(dets$image_id)))
```

Out of the 189,982 animal detections, 65,338 have been processed on the platform (34.39 %) for a total of 54,677 images (1.96 % of the total amount of images, or 4.24 % of the set of images with at least one detection, or 19.44 % of the total amount of images uploaded to the platform). This does not include the empty images also viewed as part of a set, bringing that amount to 92,826 “detections” and 82,165 images, (2.94 % of the total amount of images, or 29.21 % of the total amount of images uploaded to the platform).

Out of the 65,338 detections, 30,621 were false positives (46,87 %), and 33,237 were true positives (50.86 %), to which 1,480 false negatives were added (manually added bounding boxes, 2.26 %).

- Percentage of predictions changed (no human)

### Issues encountered

- Issues with vegetation falsely detected.

## Preliminary results

### Species ID summary

TBW

### Ordination

TBW

## Discussion

### Choice of methods

TBW

### Future plans

TBW

## Conclusion

## Acknowledgements

- Thank the volunteers for help with tagging

<!-- The following line inserts a page break  -->

\newpage

## References

<!-- The following line ensures the references appear here for the MS Word or HTML output files, rather than right at the end of the document (this will not work for PDF files):  -->

::: {#refs}
:::

\newpage

<!-- ### Colophon

This report was generated on `r Sys.time()` using the following computational environment and dependencies:

```{r}
#| label: colophon
#| cache: false

# which R packages and versions?
if ("devtools" %in% installed.packages()) devtools::session_info()
```

The current Git commit details are:

```{r}
# what commit is this file at? 
if ("git2r" %in% installed.packages() & git2r::in_repository(path = ".")) git2r::repository(here::here())  
```
 -->
